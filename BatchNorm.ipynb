{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMadb/OC7KfwZEMAttFQO9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howardatri/notes/blob/main/BatchNorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxygL1WAcPE5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class BatchNorm:\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
        "        self.num_features = num_features\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = np.ones(num_features)\n",
        "        self.beta = np.zeros(num_features)\n",
        "\n",
        "        # Running mean and variance for inference\n",
        "        self.running_mean = np.zeros(num_features)\n",
        "        self.running_var = np.ones(num_features)\n",
        "\n",
        "        # Variables for backpropagation\n",
        "        self.x_norm = None\n",
        "        self.x_centered = None\n",
        "        self.std_inv = None\n",
        "        self.var = None\n",
        "        self.mean = None\n",
        "        self.N = 0\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        \"\"\"\n",
        "        Forward pass for batch normalization.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): Input data of shape (N, D) where N is batch size and D is number of features.\n",
        "            training (bool): Whether in training mode.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Output after batch normalization.\n",
        "        \"\"\"\n",
        "        if training:\n",
        "            self.N = x.shape[0]\n",
        "            self.mean = np.mean(x, axis=0)\n",
        "            self.var = np.var(x, axis=0)\n",
        "\n",
        "            # Update running mean and variance\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
        "\n",
        "            self.x_centered = x - self.mean\n",
        "            self.var += self.epsilon\n",
        "            self.std_inv = 1 / np.sqrt(self.var)\n",
        "            self.x_norm = self.x_centered * self.std_inv\n",
        "            out = self.gamma * self.x_norm + self.beta\n",
        "\n",
        "        else:\n",
        "            # Use running mean and variance for inference\n",
        "            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
        "            out = self.gamma * x_norm + self.beta\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        Backward pass for batch normalization.\n",
        "\n",
        "        Args:\n",
        "            dout (np.ndarray): Gradient of the loss with respect to the output of batch normalization.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Gradients of the loss with respect to input (dx), gamma (dgamma), and beta (dbeta).\n",
        "        \"\"\"\n",
        "        dbeta = np.sum(dout, axis=0)\n",
        "        dgamma = np.sum(dout * self.x_norm, axis=0)\n",
        "\n",
        "        dx_norm = dout * self.gamma\n",
        "        dstd_inv = np.sum(dx_norm * self.x_centered, axis=0)\n",
        "        dvar = dstd_inv * (-0.5) * (self.var)**(-1.5)\n",
        "\n",
        "        dmean = np.sum(dx_norm * (-self.std_inv), axis=0) + dvar * (-2 / self.N) * np.sum(self.x_centered, axis=0)\n",
        "\n",
        "        dx = dx_norm * self.std_inv + dvar * (2 / self.N) * self.x_centered + dmean / self.N\n",
        "\n",
        "        return dx, dgamma, dbeta\n",
        "\n",
        "# Example usage:\n",
        "# Assume x is your input data with shape (batch_size, num_features)\n",
        "# num_features = 10\n",
        "# batch_size = 32\n",
        "# x = np.random.randn(batch_size, num_features)\n",
        "\n",
        "# bn = BatchNorm(num_features)\n",
        "\n",
        "# Forward pass in training mode\n",
        "# out_train = bn.forward(x, training=True)\n",
        "# print(\"Output in training mode:\\n\", out_train)\n",
        "\n",
        "# Forward pass in inference mode (after training)\n",
        "# out_inference = bn.forward(x, training=False)\n",
        "# print(\"Output in inference mode:\\n\", out_inference)\n",
        "\n",
        "# Example backward pass (assuming you have a gradient dout)\n",
        "# dout = np.random.randn(*out_train.shape)\n",
        "# dx, dgamma, dbeta = bn.backward(dout)\n",
        "# print(\"Gradient with respect to input:\\n\", dx)\n",
        "# print(\"Gradient with respect to gamma:\\n\", dgamma)\n",
        "# print(\"Gradient with respect to beta:\\n\", dbeta)"
      ]
    }
  ]
}