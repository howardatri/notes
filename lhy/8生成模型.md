### 8.1生成对抗网络
本章介绍生成模型（generative model）。到目前为止，我们学习到的网络本质上都是 一个函数，即提供一个输入x，网络就可以输出一个结果y。并且我们在前几章已经了解各种 各样的网络，它们可以应对不同类型的输入x和输出y。

#### 8.1.1生成器
接下来，我们将介绍另一种架构————生成模型。与先前介绍的模型不同的是，生成模 型中网络会被作为一个生成器（generator）来使用。具体来说，在模型输入时会将一个随机 变量z 与原始输入x一并输入到模型中，这个变量是从随机分布中采样得到。输入时可以采 用向量拼接的方式将x和z一并输入，或在x、z 长度一样时，将二者的加和作为输入。这 个变量z 特别之处在于其非固定性，即每一次我们使用网络时都会从一个随机分布中采样得 到一个新的z。
	我们对于该随机分布的要求是其足够简单，例如高斯分布（Gaussian distribution）、均匀分布 （uniform distribution）
所以每次有一个输入 x 的同时，我们都从随机分布中采样得 到z，来得到最终的输出y。随着采样得到的z 的不同，我们得到的输出y 也会不一样。同 理，对于网络来说，其输出也不再固定，而变成了一个复杂的分布，我们也将这种可以输出一 个复杂分布的网络称为生成器。
###### 我们为什么要需要训练生成器？为什么需要输出一个分布呢？
下面介绍一个视频预测的例子，即给模型一段的视频短片，然后让它预测接下 来发生的事情。
如果我们使用前几章介绍的基于监督学习的训练方法，我们得到的结果可 能会是的十分模糊的甚至游戏中的角色消失、出现残影的
造成该问题的原因是，我们监督学习中的训练数据对于同样的转角同时存储有角色向左 转和向右转两种输出。当我们在训练的时候，对于一条向左转的训练数据，网络得到的指示就 是要学会游戏角色向左转的输出。同理，对于一条向右转的训练数据，网络得到的指示就是学 会角色向右转的输出。但是实际上这两种数据可能会被同时训练，所以网络就会学到的是“两 面讨好”。当这个输出同时距离向左转和向右转最近，网络就会得到一个错误的结果———— 向左转是对的，向右转也是对的。
**所以我们应该如何解决这个问题呢？答案是让网络有概率的输出一切可能的结果，或者 说输出一个概率的分布，而不是原来的单一的输出**
当我们给网络一个随机 分布时，网络的输入会加上是一个z，这时输出就变成了一个非固定的分布，其包含了向左转和向右转的可能。举例来说，假设我们选择的z服从一个二项分布，即就只有0和1并且各 占50%。那么我们的网络就可以学到z采样到1的时候就向左转，采样到0的时候就向右转， 这样就可以解决了。

回到生成器的讨论中，我们什么需要这类的生成模型呢？==答案是当我们的任务需要“创造 性”的输出==，或者我们想知道一个可以输出多种可能的模型，且这些输出都是对的模型的时候。
。所以对于我们的生成模型来说，其需要能够输出一个分布，或者说多个答案。 当然在生成模型中，非常知名的就是**生成式对抗网络（generative adversarial network）**， 我们通常缩写为GAN。这一节我们就讲介绍这个生成对抗网络。
我们通过让机器生成动画人物的面部来形象地介绍GAN，首先介绍的是无限制生成（un conditional generation），也就是我们不需要原始输入 x。其对应的就是需要原始输入 x 的条件型生成（conditional generation）。如图 8.5 所示，对于无限制的 GAN，它的唯一 输入就是z，这里假设为正态分布采样出的向量。其通常是一个低维的向量，例如50、100的 维度。
我们首先从正态分布中采样得到一个向量z，并输入到生成器中，生成器会给我们一个对 应的输出——一个动漫人物的脸。我们聚焦一下生成器输出一个动漫人物面部的过程。其实 很简单，一张图片就是一个高维的向量，所以生成器实际上做的事情就是输出一个高维的向 量，比如是一个64×64 的图片（如果是彩色图片那么输出就是64×64×3）。当输入的向量z 不同的时候，生成器的输出就会跟着改变，所以我们从正态分布中采样出不同的z，得到的输 出y 也就会不同，动漫人脸照片也不同。
#### 8.1.2辨别器(discriminator)
在GAN 中，除了生成器以外，我们要多训练一个判别器（discriminator），其通常是 一个神经网络。判别器会输入一张图片，输出一个标量，其数值越大就代表现在输入的图片越 像是真实的动漫人物的图像.
判别器从本质来说与生成器一样也是神经网络，是由我们自 己设计的，可以用卷积神经网络，也可以用Transformer，只要能够产生出我们要的输入输出 即可。当然对于这个例子，因为输入是一张图片，所以选择卷积神经网络，因为其在处理图像 上有非常大的优势。
##### 过程
我们回到动漫人物图片的例子，生成器学习画出动漫的人物的过程如图8.7所示。首先， 第一代生成器的参数几乎是完全随机的，所以它根本就不知道要怎么画动漫人物，所以其画出 来的东西就是一些莫名其妙的噪音。那判别器学习的目标是成功分辨生成器输出的动漫图片。对判别器来说它只要看图片中是否有两个黑黑的眼睛 即可。接下来生成器就要通过训练调整里面的参数来骗过判别器。假设判别器判断一张图片 是不是真实图片的依据是看图片有没有眼睛，那新的生成器就需要输出有眼睛的图片。同时判别器也是会进化的，其会试图 分辨新的生成器与真实图片之间的差异。例如，通过有没有嘴巴来识别真假。所以第三代的生 成器就会想办法去骗过第二代的判别器，比如把嘴巴加上去。当然同时判别器也会逐渐的进 步，会越来越严苛，来“逼迫”生成器产生出来的图片越来越像动漫的人物。
所以生成器和判别 器彼此之间是一直的互动、促进关系，和我们所说的“内卷”一样。
GAN 最早出现在14年的一篇文章中，其作者把生成器和判别器当作是敌人，并且生成 器和判别器中间有一个对抗的关系，所以就用了一个“对抗”这个单词，当然这只是一个拟人化 的说法而已。
### 8.2生成器与辨别器的训练过程
0. 下面，我们从算法角度来解释生成器和判别器是如何运作的，如图8.8所示。生成器和判 别器是两个网络，在训练前我们要先分别进行参数初始化。
1. 训练的第一步是固定生成器，只训 练判别器。因为生成器的初始参数是随机初始化的，所以它什么都没有学习到，输入一系列采 样得到的向量给它，它的输出肯定都是些随机、混乱的图片，就像是坏掉的老式电视收不到信 号时的花屏一样，与真实的动漫头像完全不同。同时，我们会有一个很多动漫人物头像的图 像数据库，可以通过爬虫等方法得到。我们会从这个图库中采样一些动漫人物头像图片出来， 来与生成器产生出来的结果对比从而训练判别器。判别器的训练目标是要分辨真正的动漫人 物与生成器产生出来的动漫人物间的差异。具体来说，我们把真正的图片都标1，生成器产生 出来的图片都标0。接下来对于判别器来说，这就是一个分类或回归的问题。如果是分类的问 题，我们就把真正的人脸当作类别1，生成器产生出来的图片当作类别2，然后训练一个分类 器。如果当作回归的问题，判别器看到真实图片就要输出1，生成器的图片就要输出0，并且 进行0-1 之间的打分。总之，判别器就学着去分辨这个真实图像和产生出来的图像间的差异。
2. 第二步，固定判别器，训练生成器。训练 生成器的目的就是让生成器想办法去骗过判别器，因为在第一步中判别器已经学会分辨真图 和假图间的差异。生成器如果可以骗过判别器，那生成器产生出来的图片可能就可以以假乱 真。
		具体的操作如下：首先生成器输入一个向量，其可以来源于我们之前介绍的高斯分布中采样数据，并产生一个图片。接着我们将这个图片输入到判别器中，判别器会给这个图片一个打 分。这里判别器是固定的，它只需要给更“真”的图片更高的分数即可，生成器训练的目标就是 让图片更加真实，也就是提高分数。

对于真实场景中生成器和判别器都是有很多层的神经网络，我们通常将两者一起当作一 个比较大的网络来看待，**但是不会调整判别器部分的模型参数**。我们只能训练生成的部分，训练方法与前 几章介绍的网络训练方法基本一致，只是我们希望优化目标越大越好，这个与之前我们希望 损失越小越好不同。当然我们也可以直接在优化目标前加“负号”，就当作损失看待也可以，这 样就变为了让损失变小。

总结一下，GAN算法的两个步骤。步骤一，固定生成器训练判别器；步骤二，固定判别 器训练生成器。接下来就是重复以上的训练，训练完判别器固定判别器训练生成器。训练完生 成器以后再用生成器去产生更多的新图片再给判别器做训练。训练完判别器后再训练生成器， 如此反覆地去执行。
![[Pasted image 20251012205631.png]]
### 8.3 GAN的应用案例
除了产生动画人物以外，当然也可以产生真实的人脸，如图8.12产生高清人脸的技术，叫 做渐进式GAN（progressive GAN），上下两排都是由机器产生的人脸
同样，我们可以用GAN产生我们从没有看过的人脸，如图8.13所示。举例来说，先前我 们介绍的GAN中的生成器，就是输入一个向量，输出一张图片。此外，我们还可以把输入的 向量做内差，在输出部分我们就会看到两张图片之间连续的变化。比如我们输入一个向量通 过GAN 产生一个看起来非常严肃的男人，同时输入另一个向量通过GAN产生一个微笑着 的女人。那我们输入这两个向量中间的数值向量，就可以看到这个男人逐渐地笑了起来。
不过如果我们不加约束，GAN会产生一些很奇怪的图片，如图8.14所示。比如我们使用 BigGAN 算法，会产生一个左右不对称的玻璃杯子，甚至产生一个网球狗，还是很有趣的。
## 8.4 GAN的理论介绍
这一小节我们将从理论层面介绍GAN，即为什么生成器与判别器的交互可以产生人脸图 片。首先，我们需要了解训练的目标是什么。在训练网络时，我们要确定一个损失函数，然后 使梯度下降策略来调整网络参数，并使得设定的损失函数的数值最小或最大即可。
在8.1中我 们介绍了，生成器的输入是一系列的从分布中采样出的向量，生成器就会产生一个比较复杂 的分布，如图8.15所示，我们称之为PG。另外我们还有一系列的数据，这些原始的数据本身 会形成另外一个分布，我们称之为Pdata。训练的效果是希望PG 和Pdata 尽可能的相似。
$$
G^* = arg min Div(Pg,Pdata)
$$

我们再举一个一维的简单例子说明PG和Pdata，我们假设生成器的输入是一个一维的向 量，如图8.15的橙色曲线，生成器的输出也是一维的向量，如图8.15的绿色曲线，真正的数 据同样是一个一维的向量，它的分布用蓝色曲线来表示。假设每次我们输入5个点，那每一 个点的位置就会随着训练次数而改变，就会产生一个新的分布。可能本来所有的点都集中在 中间，但是通过生成器，通过一个网络里面很复杂的训练后，这些点就分成两边，变成图片中 的分布的样子。我们训练的结果是希望两个分布PG Pdata 越接近越好，即图片中的公式所示，表达的是这两个分布之间的差异，我们可以将其视 为两个分布间的某种距离，如果这个距离越大，就代表这两个分布越不像；差异越小，代表这 两个分布越相近。所以==差异==就是衡量两个的分布相似度的一个指标。
我们现在的目标就是训 练一组生成器模型中的网络参数，可以让生成的PG和Pdata之间的差异越小越好，这个最优 生成器称为==G∗==。
训练生成器的过程训练例如卷积神经网络等简单网络非常地像。对于一般的神经网络，其损失函数是可以计算的，但是对于生成器的差异，我们应该怎么处理 呢？对于连续的差异例如KL散度和JS散度是很复杂的，在实际离散的数据中，我们或许无 法计算其对应的积分。
对于GAN，只要我们知道怎样从PG 和Pdata 中采样，就可以计算得到差异，而不需要 知道实际的公式。例如，我们对于图库进行随机采样时，就会得到Pdata。对于生成器，我们 需要从正态分布中采样出来的向量通过生成器生成一系列的图片，这些图片就是PG 采样出 来的结果。所以我们有办法从PG采样，也可以从Pdata 进行采样。接下来，我们将介绍如何 只做以上采样的前提下，即不知道PG 和Pdata 的形式以及公式的情况下，如何估算得到差 异，这其中要依靠==判别器==的力量。
我们首先回顾下判别器的训练方式。首先，我们有一系列的真实数据，也就是从Pdata采 样得到的数据。同时，还有一系列的生成数据，从PG中采样出来的数据。根据真实数据和生成数据，我们会去训练一个判别器，其训练目标是看到真实数据就给它比较高的分数，看到 生成的数据就给它比较低的分数。我们可以把它当做是一个优化问题，具体来说，我们要训 练一个判别器，其可以最大化一个目标函数，当然如果我们最小化它就可以称它为损失函数。我们可以把它当做是一个优化问题，具体来说，我们要训 练一个判别器，其可以最大化一个目标函数，当然如果我们最小化它就可以称它为损失函数。 这个目标函数如图8.16所示，其中有一些 y 是从 Pdata 中采样得到的，也就是真实的数据， 而我们把这个真正的数据输入到判别器中，得到一个分数。另一方面，我们还有一些y来源 于生成器，即从PG中采样出来的，将这些生成图片输入至判别器中同样得到一个分数，再取 Log(1 −D(Y))。
![[Pasted image 20251012212013.png]]
$$
V(G,D) = \mathbb{E}_{\boldsymbol{y}\sim P_{\mathrm{data}}}[\log D(\boldsymbol{y})] + \mathbb{E}_{\boldsymbol{y}\sim P_{g}}[\log(1-D(\boldsymbol{y}))]
$$
D是discriminator网络，E是对D输出进行求期望
$$
D^* = arg max V(D,G)
$$
*分类用的是交叉墒（分类效果越好，值越低）函数D是交叉墒乘负号，所以值越大，分类效果越好。如果分布接近，那就是不管怎么训练，分类效果都不好，也就是值永远都train 不大*
我们希望目标函数V 越大越好，其中y如果是从Pdata 中采样得到的真实数据，它就要 越大越好；如果是从PG 采样得到的生成数据，它就要越小越好。这个过程在GAN提出之 初，人们将其写为这样其实还有一个缘由，就是为了让判别器和二分类产生联系，因为这个 目标函数==本身就是一个交叉熵乘上一个负号==。
negative cross entropy = training classifier
训练一个分类器时的操作就是要最小化交叉熵， 所以当我们最大化目标函数的时候，其实等同于最小化交叉熵，也就是等同于是在训练一个 分类器。有两个类别的数据，训练一个二分类的分类器，训 练后就等同于是解了这个优化问题。而图中红框里面的数值，它本身就和JS散度有关。或许 最原始的GAN 的文章，它的出发点是从二分类开始的，一开始是把判别器写成二分类的分 类器然后有了这样的目标函数，然后再经过一番推导后发现**这个目标函数的最大值和JS散度 是相关的**。
当然我们还是要直观理解下为什么目标函数的值会和散度有关。这里我们假设 PG 和 Pdata 的差距很小，就如图8.16 所示蓝色的星星和红色的星星混在一起。这里，判别器就是在 训练一个0、1 分类的分类器，但是因为这两组数据差距很小，所以在解决这个优化问题时， 就很难让目标函数V 达到最大值。但是当两组数据差距很大时，也就是蓝色的星星和红色的 星星并没有混在一起，那么就可以轻易地把它们分开。当判别器可以轻易把它们分开的时候， 目标的函数就可以变得很大。所以当两组数据差距很大的时候，目标函数的最大值就可以很 大。当然这里面有很多的假设，例如判别器的分类能力无穷大。
$$
G^* = arg min maxV(G,D)
$$
**大致上是找个一个很好的D,能够识别generator和data的差别，然后再要让这个差别最小**
我们要找两个最优化1找一个生成器，使得找生成与标准的散度的最小，2另一个最优化是找一个辨别器的objective function 越大越好
而辨别器的函数最大值与散度有关，散度又是越小越好，所以min max
最小和最大的Min Max过程就像是生成器和判别器进行互动，互相“欺骗”的过程。注意， 这里的差异函数不一定使用KL或者JS等函数，可以尝试不同的函数来得到不同差异衡量指标。

## 8.5 WGAN算法
因为要进行 MinMax 操作，所以 GAN 是很不好训练的。我们接下来介绍一个 GAN 训练的小技巧，就是著名的Wasserstein GAN（Wasserstein Generative Adversarial Network）。
在讲这个之前，我们分析下JS散度有什么问题。
首先，JS散度的两个输入PG 和Pdata 之间的重叠部分往往非常少。这个其实也是可以预料到的，我们从不同的角度来看： 图片其实是高维空间里低维的流形，因为在高维空间中随便采样一个点，它通常都没有办法 构成一个人物的头像，所以人物头像的分布，在高维的空间中其实是非常狭窄的。换个角度 解释，如果是以二维空间为例，图片的分布可能就是二维空间的一条线，也就是PG和Pdata 都是二维空间中的两条直线。而二维空间中的两条直线，除非它们刚好重合，否则它们相交 的范围是几乎可以忽略的。从另一个角度解释，我们从来都不知道PG和Pdata的具体分布， 因为其源于采样，所以也许它们是有非常小的重叠分布范围。比如采样的点不够多，就算是这 两个分布实际上很相似，也很难有任何的重叠的部分。
所以以上的问题就会对于JS分布造成以下问题：首先，对于两个没有重叠的分布，JS散 度的值都为Log2，与具体的分布无关。就算两个分布都是直线，但是它们的距离不一样，得 到的JS 散度的值就都会是Log2。因为JS散度的值是有上限的，所以当两个分布的重叠部分很大时，JS散度不好区 分不同分布间的差异。所以既然从JS散度中，看不出来分布的差异。那么在训练的时候，我 们就很难知道我们的生成器有没有在变好，我们也很难知道我们的判别器有没有在变好。所 以我们需要一个更好的衡量两个分布差异的指标。
我们从更直观的实际操作角度来说明，当使用JS散度训练一个二分类的分类器，来去分 辨真实和生成的图片时，会发现实际上正确率几乎都是100%。原因在于采样的图片根本就没 几张，对于判别器来说，采样256张真实图片和256张假的图片它直接用硬背的方法都可以 把这两组图片分开。所以实际上如果用二分类的分类器训练判别器下去，识别正确率都会是 100%。根本就没有办法看出生成器有没有越来越好。
既然是JS 散度的问题，肯定有人就想问说会不会换一个衡量两个分布相似程度的方式， 就可以解决这个问题了呢？是的，于是就有了Wasserstein，或使用Wasserstein 距离的想法。 Wasserstein 距离的想法如下，假设两个分布分别为 P 和 Q，我们想要知道这两个分布的差 异，我们可以想像有一个推土机，它可以把P 这边的土堆挪到Q这边，那么推土机平均走的 距离就是Wasserstein 距离。在这个例子里面，我们假设P 集中在一个点，Q集中在一个点， 对推土机而言，假设它要把P 这边的土挪到Q这边，那它要平均走的距离就是D，那么P 和Q的Wasserstein 距离就是 D。
但是如果是更复杂的分布，要算Wasserstein 距离就有点困难了，如图 8.19 所示。假设 两个分布分别是P 和Q，我们要把P 变成Q，那有什么样的做法呢？我们可以把P 的土 搬到Q来，也可以反过来把Q的土搬到P。所以当我们考虑比较复杂分布的时候，两种分布计算距离就有很多不同的方法，即不同的“移动”方式，从中计算算出来的距离，即推土机平 均走的距离就不一样。
我们这里先避开这个问题，先来看看Wasserstein距离有什么好处:
		Wasserstein距离 可以很好地反映两个分布的差异。从左到右我们的生成器越来越进步，但是如果同时观察判别 器，你会发现你观察不到任何规律。因为对于判别器而言，每一个例子算出来的JS散度，都是 一样的Log2，所以判别器根本就看不出来这边的分布有没有变好。
所以WGAN我们可以看到生成器从左到右越来越好
本来两个分布PG0 和Pdata 距离非常遥 远，你要它一步从开始就直接跳到结尾，这是非常困难的。但是如果Wasserstein距离，你 可以让PG0 和Pdata 慢慢挪近到一起，可以让它们的距离变小一点，然后再变小一点，最后 就可以让它们对齐在一起。所以这就是为什么我们要用Wasserstein距离的原因，因为它可以 让我们的生成器一步一步地变好，而不是一下子就变好。
**所以WGAN实际上就是用Wasserstein距离来取代JS距离，这个GAN就叫做WGAN。**
![[Pasted image 20251013191041.png]]
==解这个公式就算出Wasserstein距离==
此外还有另外一个限制。函数D必须要是一个1-Lipschitz的函数。我们可以想像成，如 果有一个函数的斜率是有上限的（足够平滑，变化不剧烈），那这个函数就是1-Lipschitz 的 函数。如果没有这个限制，只看大括号里面的值只单纯要左边的值越大越好，右边的值越小 越好，那么在蓝色的点和绿色的点，也就是真正的图像和生成的图像没有重叠的时候，我们 可以让左边的值无限大，右边的值无限小，这样的话，这个目标函数就可以无限大。这时整个 训练过程就根本就没有办法收敛。

那接下来的问题就是如何确保判别器一定符合 1-Lipschitz 函数的限制呢？其实最早刚 提出WGAN 的时候也没有什么好想法。
后来就有了一些其它的方法，`ImprovedWGAN`，它就是使用了梯度惩罚（gradient penalty）的方法，这个方法可以让判别器变成 1-Lipschitz 函数。
在真实数据这 边采样一个数据，在生成数据这边取一个样本，然后在这两个点之间取一个中间的点，然后计 算这个点的梯度，使之接近于1。
![[Pasted image 20251013191648.png]]
还有叫做谱归一化
## 8.6 训练GAN的难点与技巧
GAN 是以很难训练而闻名的，我们接下来介绍一些其中的原因和训练GAN的小技巧。有一些训练GAN的小技巧，例如Soumith、DCGAN、BigGAN等等。大家可以自己看 看相关文献进行尝试。
训练GAN 最难的一个领域其实是要拿GAN来生成文字。如果要生成一段文字那需要 一个序列到序列的模型，其中的一个解码器会产生一段文字。
我们来思考下，假设我们改变了解码器的参数，这个生成器，也就是解码器的参数， 有一点小小的变化的时候，到底对判别器的输出有什么样的影响。如果解码器的参数有一点 小小的变化，那它现在输出的分布也会有小小的变化，那因为这个变化很小，所以它对于输出 的词元不会有很大的影响。
假设输出的分布只有小小的变化，并且在取最大值的时候，或者 说在找分数最大那个词元的时候，你会发现分数最大的那个词元是没有改变的。输出的分布 只有小小的变化，所以分数最大的那个词元是同一个。那对于判别器来说，它输出的分数是没有改变的。判别器输出也不会改变，所以你根本就没有办法算微分，也根本就没有办法做 梯度下降。
和cnn里的max pooling区分:
因为cnn里是求max()这里是求argmax(),一个可导一个不可以
当然就算是不能做梯度下降，我们还是可以用强化学习的方法来训练生成器。但 是强化学习本身是以难训练而闻名，GAN也是以难训练而闻名，这样的东西加在一起，就会 非常非常地难训练。

*直到有一篇文章叫做ScratchGAN，不需要预训练（pre-training），可以直接从随机的 初始化参数开始，训练生成器，然后让生成器可以产生文字。它的方法是调节超参数，并且 加上一些训练技巧，就可以从零开始训练生成器。*
此外，其实有关生成式的模型不是只有GAN而已，还有其他的比如VAE，比如流模型 等等，这些模型都有各自的优缺点。当然，就假设目前想要训练一个生成器，想让机器可以生 成一些东西还是那有很多方法，可以用GAN，可以VAE，也可以用流模型。但是如果我们想 要产生一些图片，那就最好用GAN，因为GAN是目前为止比较好的生成式的模型，它可以 产生最好的图片。但是如果想要产生一些文字，那就只有用VAE或者流模型，因为GAN在 产生文字的时候，还是有一些问题。

## 8.7 GAN 的性能评估方法
接下来我们介绍GAN 的评估方法，也就是我们产生出来的生成器它好或者是不好。要 评估一个生成器的好坏，最直觉的做法也许是找人来看生成器产生出来的图片到底像不像真 实的图片。所以其实很长一段时间，尤其是人们刚开始研究生成式技术的时候，很长一段时间 没有好的评估方法。
针对特定的一些任务，是有办法设计一些特定方法的。那如果是更一般的案例，比如它 不一定是产生动画人物的，它专门产生猫、专门产生狗、专门产生斑马等等，那我们怎么知道 它做得好不好呢？![[Pasted image 20251013203038.png]]
其实有一个方法，是训练一个图像的分类系统，然后把GAN产生出来的图片输入到这 个图像的分类系统里面，看它产生什么样的结果，如图8.25所示。这个图像分类系统的输入 是一张图片，输出是一个概率分布。如果这个概率分布越集中，就代表现在产生的图片可能越好。如果生成出来的 图片是一个四不像，图像识别系统就会非常地困惑，它产生出来的这个概率分布就会是非常 平均地分布。
**这个就是Quality，看一张图片**
这个是靠图像识别系统来判断产生出来的图片好坏，这是一个可能的做法，但是光用这 个做法是不够的。光用这个做法会被一个叫做**模式崩塌（modecollapse）** 的问题骗过去。模 式崩塌是指在训练GAN的过程中遇到的一个状况，假设如图8.26蓝色的星星是真正的数据 的分布，红色的星星是GAN的模型的分布。我们会发现生成式的模型它输出来的图片来来 去去就是那几张，可能单一张拿出来你觉得好像还做得不错，但让它多产生几张就露出马脚， 产生出来就只有那几张图片而已，这就是模式崩塌的问题.
![[Pasted image 20251013203258.png]]
发生模式崩塌的原因，从直觉上理解可以想成这个地方就是判别器的一个盲点，当生成 器学会产生这种图片以后，它就永远都可以骗过判别器，判别器没办法看出来图片是假的。
不过模型崩塌这种问题，我们至少是知道有这个问 题，是可以看得出的，生成器总是产生这张脸的时候，你不会说你的生成器是个好的生成器。 但是有一些问题是你不知道的并且更难侦测到的，即你不知道生成器产生出来的图片是不是 真的有多样性。 
这个问题叫做==模式丢失==，指GAN能很好地生成训练集中的数据，但难以生成非训练集的数据，“缺乏想象力”。你的产生出来的数据，只有真实数据的一部分。
事实上今天这些非常好的GAN，BGAN、ProgressGAN等等可以产生非常真实 人脸这些GAN，多多少少还是有模式丢失的问题。
![[Pasted image 20251013204222.png]]
虽然存在以上模式坍塌、模式丢失等等的问题，但是我们需要去度量生成器产生出来的 图片到底多样性够不够。有一个做法是借助我们之前介绍过的图像分类，把一系列图片都丢 到图像分类器里，看它被判断成哪一个类别，如图8.27所示。每张图片都会给我们一个分布， 我们将所有的分布平均起来，接下来看看平均的分布长什么样子。如果平均的分布非常集中， 就代表现在多样性不够，如果平均的分布非常平坦，就代表现在多样性够了。

当我们用这个图像分类器来做评估的时候，对于结果的多样性和质量好像是有点互斥的。
**这个是Diversity，看一堆图片。和前面的Quality区分，这个越平坦越好**
目前研究人员通常会采取另外一个评估方式，叫FréchetInceptiondistance (FID)。具体来讲，先把生成器产生出来的人脸图片，丢到InceptionNet 里面，让 Inception 网络输出它的类别。这里我们需要的不是最终的类别，而是进入Softmax之前的隐藏层的输出向量，这个向量的维度是上千维的，代表这个图片，如图8.28所示。
![[Pasted image 20251013204811.png]]
这个向量其实非常高维度，甚至是上 千维的，我们就把它降维后画在二维的平面上。蓝色点是GAN的生成器产生出来的图片，它 丢到Inception 网络以后进入 Softmax 之前的向量。接下来，我们假设真实的图片和生成的 图片都服从高斯分布，然后去计算这两个分布之间的Fréchet的距离。这个距离越小越好
这里还有几个细节问 题，首先，假设为高斯分布没问题吗？会有问题。 另外一个问题是如果要准确的得到网络的分布，那需要 产生大量的采样样本才能做到，这需要一点运算量，也是做FID不可避免的问题。
FID 算是目前比较常用的一种度量方式，那有一篇文章叫做“AreGANs Created Equal? A Large-Scale Study”，从文章的结果来看所有的GAN都差不多，那所以与GAN 有关的研究都是白忙一场吗？事实上也未必是如此，这篇文章做实验的时候不同的GAN用的 网络架构都是同一个，只是疯狂调参而已，调随机种子和学习率而已。网络架构还是同一个， 所以是不是有某些网络架构，某些种类的GAN会不会在不同的网络架构上表现得比较稳定。 这些都有待研究。
此外，还有一个状况。假设GAN产生出来的图片，跟真实的图片长得一模一样，那此时 FID 会是零，因为两个分布是一模一样的。
所以GAN的评估是非常地困难 的，还甚至如何评估一个生成器做得好不好是一个可以研究的题目。
## 8.8 条件型生成
下面，我们要介绍条件型生成（conditional generation）。我们之前讲的 GAN 中的生 成器，它没有输入任何的条件，它只是输入一个随机的分布，然后产生出来一张图片。我们现 在想要更进一步的是希望可以操控生成器的输出，我们给它一个条件x，让他根据条件x跟 输入z 来产生输出y。

具体怎么做条件型的GAN呢？我们现在的生成器有两个输入，一个是正态分布中采样 出来的z，另一个是条件x也就是一段文字，然后它会产生出来一个y，也就是一张图片。同 时，我们需要一个判别器。在条件型的GAN里面，就要做有点不一样的 设计，也就是**判别器**不是只吃图片y，同时还要吃条件x。一般的训练是需要这个成对的标注数据的。
那其实在实际操作中，只是拿这样的负样本对和正样本对来训练判别器，得到的结果往 往不够好。往往还需要加上一种不好的状况：已经产生好的图片但是文字叙述配不上的状况。
条件型GAN还有很多应用，比如给GAN听一段声音，然后产生一个对应的图片。比如 说给它听一段狗叫声，GAN可以画出一只狗。这个应用的原理跟刚才讲的文字变图像是一样 的，只是输入的条件变成声音而已。
## 8.9 Cycle GAN
这一小节，我们要介绍一个GAN的另一个有趣的应用，就是把GAN用在无监督学习 上。到目前为止呢，我们介绍的几乎都是监督学习，即我们要训练一个网络，其输入是x，输 出为y，并且我们需要成对的数据才有办法训练网络。但是我们可能会遇到一个状况是，我们 有一系列的输入和输出，但是x和y之间并没有成对的关系，也就是说我们没有成对的数据。
举一个例子，比如图像风格转换的情况我们就没有成对的数据。
对于图像风格转换，我们可能一点成对的数据都没 有。那么在这种状况下，还有没有办法训练一个网络输入一个x产生一个y呢？这个时候就 可以用到GAN，在这种完全没有成对数据的情况下进行学习。
![[Pasted image 20251014215952.png]]
如图8.30 ，这这个是我们之前在介绍无条件生成的时候所使用的生成器架构，输入是一个 高斯的分布，输出可能是一个复杂的分布。现在稍微转换一下我们的想法，输入不是高斯分 布，而是x域的图片的分布，y域是图片的分布。我们完全可以套用原来的GAN的想法。这个时候我们的判别器就要改一下， 判别器不再是只输入y域的图片，而是同时输入x域的图片和y域的图片，然后输出一个数 值，这个数值代表这两张图片是不是一对的。
这整个过程与之前的GAN没有什么区别，但是仔细想想看只是套用原来的GAN训练， 好像是不够的。因为我们要做的事情是要让这个生成器输出一张y 域的图，但是它输出的y 域的图一定要跟输入有关系吗？==此处我们没有做任何的限制==，所以生成器也许就把这张图片 当作一个符合高斯分布的噪音，然后不管你输入什么它都无视它
那怎么解决这个问题呢？怎么强化输入与输出的关系呢？我们在介绍条件型GAN的时 候，提到说假设判别器只看y，那它可能会无视生成器的输入，所以产生出来的结果不是我们 要的。所以我们要让判别器看x和y，这样才可以让生成器学到x和y之间的关系。但是目 前如果我们要从没有样本对的数据中学习，我们也没有办法直接套用条件型GAN的想法，因 为在条件型GAN 里面我们是有成对的数据的，可以用这些成对的数据来训练的判别器。
为了解决这个问题，我们可以使用**循环生成对抗网络（Cycle GAN）。**
![[Pasted image 20251014220231.png]]
具体来说，在CycleGAN中，我们会训练两个生成器。第一个生成器是把x域的图变成 y 域的图，第二个生成器它的工作是看到一张y域的图，把它还原回x域的图。在训练的时 候，我们会增加一个额外的目标，就是我们希望输入一张图片，其从x域转成y域以后，要 从y 域转回原来一模一样的x域的图片。
因为这边有一个循环，从x到 y 再从y 回到x，所以它是一个循环，所以被称为CycleGAN。
那加入了第二个生成器以后，对于前面这个第一个的生成器来说，它就不能够随便产生 与输入没有关系的人脸了。
	另外有一个问题，我们需要只保证第一个生成器的输出和输入有一定的关系，但是我们 怎么知道这个关系是所需要的呢？
	但实际上使用CycleGAN的时候，这种状况没有那么容易出 现。
另一个角度，Cycle GAN 可以是**双向的**，如图8.32所示。我们刚才有一个生成器，输入 y 域的图片，输出x域的图片，是先把x域的图片转成y，在把y转回x。在训练cycleGAN 的时候，其实可以同时做另外一个方向的训练，也就是把橙色的生成器给它y域的图片，让 它产生x域的图片。然后在让蓝色的生成器把x域的图片还原回原来y域的图片。同时我们 依然希望输入跟输出越接近越好，所以一样要训练一个判别器，这个判别器是x域的判别器， 它是要看橙色生成器输出的图片像不像是真实人脸的图片。这个橙色的生成器它要去骗过这 个Dx 判别器。这个合起来就是CycleGAN。
![[Pasted image 20251014220438.png]]
除了Cycle GAN以外，还有很多其他的可以做风格转换的GAN，比如DiscoGAN、Dual GAN 等等，如图8.33所示。这些GAN的架构都是类似的，一样的想法。













