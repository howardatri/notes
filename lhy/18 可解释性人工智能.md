
可解释性很重要: 举例来说，银行可能会用机器学习的模型来判断要不要贷款给某一个客 户，但是根据法律的规定银行作用机器学习模型来做自动的判断它必须要给出一个理由。所 以这个时候，我们不是只训练机器学习模型就好，我们还需要机器学习的模型是具有解释力 的。机器学习未来也会被用在医疗诊断上，但医疗诊断是人命关天的事情，如果机器学习的 模型只是一个黑箱，不会给出诊断的理由的话，那我们又要怎么相信它做出的是正确的判断 呢。
有人可能会想说我们之所以这么关注可解释性的机器学习的议题，也许是因为深度的网 络本身就是一个黑箱。我们能不能够用其它的机器学习的模型呢？如果不要用深度学习的模 型，而改采用其他比较容易解释的模型会不会就不需要研究可解释性机器学习了呢。举例来 说，假设我们都采用线性模型，它的解释的能力是比较强的，我们可以轻易地知道根据一个 线性模型里面的每一个特征的权重，知道线性的模型在做什么事。所以训练完一个线性模型 后，我们可以轻易地知道它是怎么得到它的结果的。但是线性模型的问题是它没有非常地强 大，它的表达能力是比较弱的。
就像在路灯下找钥匙、钥匙其实不在路灯下，我们不能因为黑箱就放弃使用

## 18.2 决策树模型的可解释性
我们首先介绍一下一个比较简单的机器学习模型，其在设计之初就已经有了比较好的可 解释性，这个模型就是决策树模型。决策树相较于线性的模型，它是更强大的模型。而决策树 的另外一个好处，相较于深度学习它具有良好的可解释性。比如从决策树的结构，我们就可以 知道模型是凭借着什么样的规则来做出最终的判断。所以我们希望从决策树模型进行可解释 性的研究，再扩展到其他机器学习模型，甚至深度模型。
我们首先简单介绍一下决策树，它有很多的节点，那每一个节点都会问一个问题，让你决 定向左还是向右。最终当你走到节点的末尾，即叶子节点的时候，就可以做出最终的决定。因 为在每一个节点都有一个问题，我们看那些问题以及答案就可以知道现在整个模型凭借着什 么样的特征如何做出最终的决断。所以从这个角度看来，决策树它既强大又有良好的可解释 性。那我们是不是就可以用决策树来解决所有的问题呢？其实不是的，它是一个树状的结构， 那我们可以想像一下，如果特征非常地多，得到决策树就会非常地复杂，就很难去解释它了。
另外一方面，我们是怎么实际使用决策树这个技术的呢？很多同学都会说，这个打Kaggle 比赛的时候，深度学习不是最好用的，决策树才是最好用的，决策树才是Kaggle比赛的常胜 将军。但是其实当你在使用决策树的时候，并不是只用一棵决策树，你真正用的技术叫做随 机森林。真正用的技术其实是好多棵决策树共同决定的结果。一棵决策树可以凭借着每一个 节点的问题和答案知道它是怎么做出最终的判断的，但当你有一片森林的时候，你就很难知 道说这一片森林是怎么做出最终的判断的。所以决策树也不是最终的答案，并不是有决策树， 我们就解决了可解释性机器学习的问题。

## 18.3 可解释性机器学习的目标

我们首先应该定义可解释性的目标是什么。或者 说什么才是最好的可解释性的结果呢？很多人对于可解释性机器学习会有一个误解，觉得一 个好的可解释性就是要告诉我们整个模型在做什么事。我们要了解模型的一切，我们要知道 它到底是怎么做出一个决断的。但是这件事情真的是有必要的吗？虽然我们说机器学习模型， 深度网络是一个黑盒子，不能相信它，**但世界上有很多黑盒子，比如人脑也是黑盒子**。我们其 实也并不完全知道，人脑的运作原理，但是我们可以相信，另外一个人做出的决断。那为什么 深度网络是一个黑盒子，我们就没有办法相信其做出的决断呢？我们可以相信人脑做出的决 断，但是我们不可以相信深度网络做出的决断，这是为什么呢？
举了个哈佛大学的心理学实验，需要理由才会接受。你只要讲出一个理由，就算你的理由是因为我需要先印大家也会接 受。
所以会不会可解释性机器学习也是同样的道理。在可解释性机器学习中，好的解释就是 人能接受的解释，**人就是需要一个理由让我们觉得高兴**。因为很多人听到，深度网络是一个黑 盒子他就不爽，但是你告诉他说这个是可以被解释的，给他一个理由，他就高兴了。所以或许 好的解释就是让人高兴的解释。其实这个想法，这个技术的进展是蛮接近的。
## 18.4 可解释性机器学习中的局部解释
- 局部解释: why do you think this image is a cat?
- 全局解释: what does a "cat" look like?
可解释性机器学习可以被分成两大类，第一大类叫做局部的解释，第二大类叫做全局的 解释。
根据某一张图片来回答问题， 这个叫做局部的解释。我们并不是针对任何一张特定的图片来进行分 析，我们是想要知道有一个模型它里面有一些参数的时候，对这些参数而言什么样的东西叫 作一只猫，这个叫做全局解释。
![[Pasted image 20251017222828.png]]
接下来我们先来看第一大类，第一大类是为什么你觉得一张图片是一只猫。再具体一点 些，给机器一张图片，它知道图片是一只猫的时候，到底是这个图片里面的什么东西让模型觉 得它是一只猫。假设模型的输入叫做x，这个x可能是一张图片，可 能是一段文字，可能是一段音频，可能是一段视频，可能是一段时间序列的数据。其可以拆成 多个部分，x可以拆成x1到xn，这些部分对应起来可能是像素，也可能是文字，也可能是音 频的频谱，也可能是视频的每一帧，也可能是时间序列的每一个时间点。在这些部分里面，哪 一个对于机器做出最终的决断是最重要的？

如何知道一个部分的重要性呢？基本的原则是我们把所有的部分都拿出来，把每一个部 分做改造或者是删除。如果我们改造或删除某一个部分以后，网络的输出有了巨大的变化，就知道没他不行。
我们再使用图像举例，想要知道一个图像里每一个区域的 重要性的时候，就将这个图片输入到网络里。接下来在这个图片里面不同的位置放上灰色的 方块，当这个方块放在不同的地方的时候，网络会输出不同的结果。比如一只狗的图片，当我 们把灰色的方块移动到狗的脸上的时候，我们网络就不觉得它看到一只狗；但如果把灰色的 方块放在狗的四周，这个时候机器就觉得它看到的仍然是狗。所以模型就知道它不是看到球， 觉得它看到狗，也不是看到地板、墙壁，才觉得看到的是狗，而是真的看到这个狗的面部。所 以这个是最简单的，知道每一个部分的重要性的方法。

还有一个更进阶的方法，即计算梯度，如图18.2所示。具体来讲，假设我们有一张图片， 我们把它写为x1 到xN。这边的每一个xi 代表一个像素。
![[Pasted image 20251017222844.png]]
接下来我们去计算这张图片的损 失，损失用e来表示。这个e是把这张图片输入到模型中，模型输出的结果与正确答案的差 距（我们用交叉熵表示）。其数值越大，就代表现在识别的结果越差。如何知道每一个像素的 重要性呢？我们可以将每一个像素做一个小小的变化，加上一个∆x，再输入到模型里面看一 下损失会有什么样的变化。如果把某一个像素做小小的变化以后，模型输出的损失就有巨大 的变化，就代表这个像素对图像的识别是重要的，反之如果加了∆x，这个∆e趋近于零，就 代表这个位置，这个像素对于图像识别而言可能是不重要的。
所以我们可以用∆e和∆x的比 值来代表这一个像素xN 的重要性。而事实上比值这一项，就是损失关于xN 的偏导数，也就 是 ∂e ∂x 。那这个比值越大，就代表 xN 越重要。当我们把每一个图片里面每一个像素它的这个 比值都算出来后，我们就得到一个图，这个图就叫做显著图（saliency map）。图18.2中，上 面是原始图片，下面黑色有亮白色点的就是显著图，越亮白色的点，就代表这个像素越重要。
举了个宝可梦数码宝贝，jpg jpeg 区别
再举一个真实的案例，有一个基准语料库叫做PASCALVOC2007，里面有各式各样的 物体，有人、狗、猫、马、飞机等等。机器要学习做图像分类，当它看到图中这张图片它知道 是马，如图18.3 所示。如果我们看显著图的话，就会发现机器觉得这张图片是马的原因，是 因为图片的左下角有一串英文，这串英文是来自于一个网站，这个网站里面有很多马的图片， 左下角都有一样的英文，所以机器看到左下角这一行英文就知道是马，它根本不需要学习马 是长什么样子。所以在这个真实的应用中，在基准语料库中，类似的状况也是会出现的。
这告诉我们，可解释性AI是一个很重要的技术，否则我们不知道机器是怎么判断的，我们就 不知道它是不是在作弊，或者是不是有什么问题。

其实可以把可解释性机器学习的显著图画得更好，可以使用一种叫做SmoothGrad的方 法，如图18.4 所示。这张图片是羚羊，所以我们希望机器会把它主要的精力集中在瞪羚身上。 那如果我们用刚才我们讲的方法直接画显著图的话，得到的结果可能是中间图的样子。其确 实在羚羊附近有比较多亮的点，但是在其他地方也有一些噪声让人看起来有点不舒服，所以 就有了SmoothGrad 这个方法。SmoothGrad 会让你的这个显著图，上面的噪声比较少，在这 个例子中就是多数的亮点都集中在羚羊身上。那SmoothGrad 这个方法是怎么做呢？其实就 是在图片上面加上各种不同的噪音，加不同的噪声就是不同的图片了。接着在每一张图片上 面都去计算显著图，所以有加100种噪声，就有100张显著图，平均起来就得到SmoothGrad 的结果。
![[Pasted image 20251017224002.png]]

当然梯度并不是万能的，梯度并不完全能够反映一个部分的重要性，举一个例子以供参 考，如图18.5 所示。横轴代表的是大象鼻子的长度，纵轴代表这个生物是大象的可能性。我 们都知道大象的特征是长鼻子，所以鼻子越长，这个生物是大象的可能性就越大。但是当鼻子 长到一定程度以后，再长鼻子也不会让这个生物变得更像大象了。
![[Pasted image 20251017224116.png]]
这个时候如果计算鼻子长度对是大象可能性的偏导数的话，在这个地方得到的偏导数可 能会趋近于0。所以如果仅仅看梯度，仅仅看显著图，可能会得到一个结论是鼻子的长度对是 不是大象这件事情是不重要的，鼻子的长度不是判断是否为大象的一个指标，因为鼻子的长 度的变化，对是大象的可能性的变化是趋近于0的。
但是事实上，我们知道鼻子的长度是一 个很重要的指标，鼻子越长，这个生物是大象的可能性就越大。所以仅仅看梯度和偏导数的结 果，可能没有办法完全告诉我们一个部分的重要性。所以有其他的方法被提出，比如积分梯度 （integrated gradients）等等。
刚才我们是看网络输入的哪些部分是比较重要的，那接下来我们要问的下一个问题是当 我们给网络一个输入的时候，它到底是如何去处理这个输入的，并得到最终的答案的。
这里也 有不同的方法，第一个方法最直觉的，就是人眼去看，看看网络到底是怎么处理这个输入的。 我们举一个语音的例子，如图18.6所示。这个网络的功能是输入一小段声音，输出这个声音 是属于哪一个韵母，属于哪一个音标等等。假设该网络第一层有100个神经元，第二层也有 100 个神经元。那第一层和第二层的输出就可以看作是100 维的向量。通过这些分析这些向 量，也许我们就可以知道一个网络里面发生了什么事。但是100维的向量不容易分析，所以 我们可以把100维的向量把它降到二维，比如使用PCA或者t-SNE等等方法。把100维降 到二维以后就可以画在图上，就可以直接可视化它。这个时候我们就可以看到，这个网络到底 是怎么处理这个输入的，它到底是怎么把这个输入变成最后的输出的。
再举一个语音的例子，那这个例子来自于一篇Hinton 的文章。首先我们把模型的输入， 就是声音特征，也就是MFCC拿出来把它降到二维，画在二维平面上，如图18.7所示。这个图上每一个点代表一小段声音信号，每一个颜色代表了某一个讲话的人。其实我们输入给 网络的数据有很多句子是重复的，比如A、B、C这三个人都说了Howareyou 这句话，很 多人说了一样的句子。但从声音特征上，就算是不同的人念同样的句子，我们从声音特征上 并不能分别出来不同的人。所以有的人就会觉得语音识别太难了，因为不同的人说同样的话， 声音特征都是一样的。但是当我们把网络拿出来可视化时候，结果就不一样了。右边的图是 第8个隐藏层的输出，我们会发现每一条代表同样内容的某一个句子，所以不同人说同样的 内容在MFCC上看不出来，但是它通过了8层的网络之后，机器知道说这些话是同样的内容 了，所以最后模型就可以得到精确的分类结果。
![[Pasted image 20251017233557.png]]
除了用人眼观察可视化以外，还有另外一个技术叫做探针（probing）。简单来说，就是 用探针去插入这个网络，看看会发生什么事。
## 18.5 可解释性机器学习中的全局解释
介绍完局部解释，接下来介绍全局的解释。局部的解释是给机器一张照片，让它告诉我们 说看到这张图片，它为什么觉得里面有一只猫。与此不同的是，全局解释并不是针对特定某一 张照片来进行分析，而是把我们训练好的模型拿出来，根据这个模型里面的参数去检查它的 特性。
举了mnist手写数字：
	下面我们使用MNIST 手写数字识别的例子继续解释。我们训练好了一个卷积神经网络， 它的结构如图18.12 所示。接着继续使用 MNIST 数据集训练一个分类器，这个分类器的功 能是给它一张图片它会判断其为0∼9的哪一个数字。训练好这个分类器以后，我们把它的第 二个卷积层里面的滤波器取出，找出每一个滤波器对应的X∗，也就是每一个滤波器想要看到 的模式。图18.12 的左侧每一张图片就是一个X∗，每一张图片都对应到一个滤波器。比如左 上角的第一张图片就是滤波器1想要挖掘的模式。第二张图片就是滤波器2想要挖掘的模式， 以此类推，我们一共画了12个滤波器，所以一共有12张小图片。
	从这些模式里面我们可以发现第二层的卷积确实是去挖掘一些基本的模式，比如说类似 笔画的东西，比如横线、直线、斜线，而且每一个滤波器都有自己独有的想要挖掘的模式。
	那如果我们希望看到的是比较像是人想像的数字应该要怎么办呢？方法是在解优化问题 的时候，加上更多的限制。我们已经知道数字可能是长什么样子，所以要把这个限制加到这个 优化的过程里。举例来说，我们现在不是要找一个X，让yi 的分数最大，而是要找一个X， 让yi 的分数最大，同时让yi还有R(X)的分数都越大越好。这里的R(X)是一个正则项，它 的作用是让X更像是一个我们人类认识中的数字。这个R(X)可以是很多种形式，比如说我 们可以让R(X) 是X 的像素值的平方和，也就是说我们希望X 里面的每一个像素值都不要 太大。

如果我们希望使用全局解释性来看到非常清晰的图片的话，有一个方法就是训练一个图 像生成器。我们可以用GAN或者VAE等等生成模型训练一个图像的生成器。图像的生成器 的输入是一个从高斯分布里采样出来的低维度的向量叫做z，输入到这个图像生成器以后，它 输出就是一张图片X。这个图像生成器我们使用G来表示，所以输出的图片X,我们就可以 写成X=G(z)，如图18.14 所示。那么如何拿这个图像生成器来帮助我们反推一个图像分类 器中它所想像的某一种类别长什么样子呢？很简单，把这个图像生成器和图像分类器接在一 起即可。


## 18.6 扩展与小结
那其实可解释性机器学习还有很多的技术，比如说我们可以用一些可解释性的模型来替 代黑盒模型，比如说我们可以用线性模型来替代神经网络模型，如图18.15所示。也就是用一 个比较简单的模型想办法去模仿复杂的模型的行为，如果简单的模型可以模仿复杂模型的行 为，只需要去分析那个简单的模型，也许就可以知道复杂的模型在做什么。
这里可能会有疑问，一个线性模型有办法去模仿一个黑盒的行为吗？我们在之前介绍过， 有很多的问题是神经网络才做得到而线性模型做不到的。所以黑盒可以做到的事情线性模型 不一定能做到。这里有一个特别知名的工作称为局部可解释的模型无关解释（Local Inter pretable Model-agnostic Explanations，LIME）。这个方法也不能用线性模型去模仿黑盒全部 的行为，但是它可以用线性模型去模仿黑盒在一个小的区域内的行为，这个区域内的行为是 可以用线性模型去模仿的，所以它叫局部的可解释性。
这一章主要和大家介绍了可解释性机器学习的两 个主流的技术，局部的解释和全局的解释。



