本章介绍人工智能中的对抗攻击。之前我们已经了解了各式各样的神经网络，这些神经 网络对于不同的输入输出类别都有非常高的正确率，我们期待可以把这些技术用在真正的应 用上。但是把这些网络真正的使用起来，仅仅提高它们的正确率是不够的。它们需要能够应 付来自人类的恶意，也就是说，它们需要能够对抗来自外界的攻击。有时候神经网络的工作 是为了要检测一些有恶意的行为，**它要检测的对象会去想办法骗过网络**，所以我们不仅要在 一般的情况下得到高的正确率，还需要它在有人试图想要欺骗它的情况下也得到高的正确率。
## 12.1 对抗攻击简介
我们首先看看所谓人类的恶意是什么样子，以下是一个真正的例子，如图12.1所示。之 前我们已经训练了图像识别模型，给它一张照片，它可以告诉我们这张照片属于什么类别。那 我们要做的攻击就是在这张照片上面加入一个非常小的噪声，具体方法是一张照片可以被看 作是一个矩阵，我们在这个矩阵的每一个数据上都加入一个小小的噪声，然后把这个加入噪 声以后的照片输入到网络中查看输出的分类结果。
当然，如果我们加入的只是一般的噪声，网络并不一定会犯错，如图12.3所示。左上角 是原来的图片，我们现在加入一个肉眼可见的噪声，如左下角所示，这个时候ResNet还是可 以正确地识别出这是一只猫。只不过换了品种。那我们把噪声加得更大一点，变为右上角的图 片，这个时候ResNet 会说这是波斯猫，这个猫看起来毛茸茸的，所以因此ResNet觉得它看 到了波斯猫。那如果噪声再加更大一点，如右下角，这个时候ResNet就会识别为壁炉，因为 它觉得前面的噪声是屏风，而后面这个橙色的猫就是火焰。它虽然犯错，但是错的是有尊严 的，是有道理的。

## 12.2 如何进行网络攻击
![[Pasted image 20251018223910.png]]
那L2 范数和L-无穷范数到底哪一个在攻击的时候是比较好的距离呢？
## 12.3 快速梯度符号法
所谓的攻击还有很多不同的变形，不过大同小异，它们通常要么是限制不一样，要么是 优化的方法不一样。接下来再介绍一个最简单的攻击的方法，这个方法叫做快速梯度符号法， 全称是快速梯度符号法（Fast Gradient Sign Method，FGSM）。本来一般我们在做梯度 下降的时候，需要迭代更新参数很多次，但是这个方法只需要更新一次参数就可以了，原理如 图12.7 所示。
具体来讲，FGSM 将原始的梯度 g 做了一个特别的设计，不是直接使用梯度下降的值， 而是取一个符号函数，也就是值大于0，我们就输出1，值小于0，就输出−1。所以加了符 号函数以后，这个梯度g向量要么是1要么是−1。对于学习率η，直接将其设置为ε **(上面的阈值)**，这样 会得到的效果是我们攻击完以后，更新后的x一定落在这个蓝色框四个角落。
因为梯度g要 么是1要么是−1，再乘以ε后，x要么往右边移动ε，要么往左边移动ε，要么往上边移动 ε，要么往下边移动ε。它一定会挪到这个方形的四个角落的地方，所以这个攻击方法就是这 么简单。
## 12.4 白盒攻击与黑盒攻击
我们以上介绍的其实都是比较有代表性的白盒攻击，也就是说我们知道模型的参数，知 道模型的结构，知道模型的输入输出，知道模型的损失函数，知道模型的梯度等等。我们知道 模型的参数，所以才有办法计算梯度，才有办法去在图像上加上噪声。那像这种知道模型参数的攻击叫做白盒攻击。但是因为白盒攻击需要知道网络的参数，所以也许白盒击不是很危险， 因为很多线上的服务模型，我们很难不知道它的参数是什么，所以也许要攻击一个线上的服 务并没有那么容易。换一个角度，其实如果要保护我们的模型不被别人攻击，我们只要注意不 要随便把自己的模型放到网络上公开让大家取用就好。
其实我们想简单了，因为我们的模型参数是可以通过一些方法来反推出来的，这个方法 叫做黑盒攻击，也就是说我们不知道模型的参数，但是我们可以通过一些方法来反推出来。
具 体来讲，如果有一个模型我们无法获知其中的具体参数，但是知道该网络是用什么样的训练 数据训练出来的话，我们就可以去训练一个代理网络，让该网络来模仿我们要攻击的对象。如 果它们都使用同样的训练数据训练模型的话，也许它们就会有一定程度的相似度。如果代理 网络与要被攻击的网络有一定程度的相似的话，那我们只要对代理网络进行攻击，也许原始 的网络也会被攻击成功。整个过程如图12.9所示。
如果我们没有训练数据，并且不知道被攻击对象是用什么样的训练数据的话怎么办呢？其实也很简单，我们就直接将一些现有的图片输入进需要被攻击的模型，然后看看它会输出什 么，然后再把输入输出的成对数据拿去训练一个模型的话，我们就有可能可以训练出一个类 似的网络，也就是我们的代理网络了，我们再对于代理网络进行攻击即可。
![[Pasted image 20251018225310.png]]
黑盒攻击相对来说还是很容易成功的，如图12.10 所示，这是一篇论文的结果。有5个 不同的网络架构，ResNet-152、ResNet-101、ResNet-50、VGG-16 和 GoogLeNet。每一列代 表要被攻击的网络，每一行代表代理网络。对角线的地方代表是代理网络和要被攻击的网络 是一模一样的，所以这个情况就不是黑盒攻击，对角线的地方其实是白盒攻击。例如，你使用 ResNet-152 当做代理网络攻击一模一样的网络，那其实攻击就很容易成功了。

如果要增加黑盒攻击的成功率，我们可以使用集成学习的方法，也就是使用多个网络来 攻击，这样的话，我们就可以提高攻击的成功率。
![[Pasted image 20251018225616.png]]
如图12.11所示，这里有5个网络，我们可 以使用这5个网络来攻击，然后看看攻击的成功率会不会提高。每一列还代表要被攻击的网 络，那每一个行有所不同，你会发现这个每一个模型的名字前面放了一个减号，其代表把这5 个模型都集合起来，但拿掉ResNet-152，找一张图像攻击这4个网络。我们观察图12.11，与 之前不同，非对角线的地方是白攻击，非对角线模型正确率都变成0%，白盒攻击依然非常容 易成功。对角线的地方是黑盒攻击，比如我们要攻击ResNet-152，但我们没有用ResNet-152， 但是用了另外4个网络，所以对角线的地方才是黑盒攻击。所以使用集成的方式进行攻击的 时候，黑盒攻击的成功率也是非常高的，都低于6%。
那为什么黑盒攻击非常容易成功呢？下面介绍一个信服度较高的结论，它基于一个实验。
图12.12 上面的原点代表一张小丑鱼的图片，横轴和纵轴分别是把这张图片往两个不同的方 向移动。横轴是在VGG-16 上面可以攻击成功的方向，纵轴表示一个随机的方向。我们可以 观察到横轴是让VGG-16 可以攻击成功的方向，在其他的网络上面它们中间深蓝色的区域都 很相近，其表示会被识别成小丑鱼的图片的范围。也就是说如果把这个小丑鱼的图片加上一 个噪声这个矩阵在高维的空间中横向移动，基本上网络还是会觉得它是小丑鱼的图片。但是 如果是往可以攻击成功VGG-16 的方向来移动的话，那基本上其他网络也是有很高的机率可 以被攻击成功的。对于小丑鱼这一个类别，它在攻击的方向上，可移动的范围特别窄，只要把 这张图片稍微移动一下它就掉出会被识别成小丑鱼的区域范围之外了。
对每一个网络来说，攻击的方向对不同的网络影响都是很类似的。所以有些人主张攻击 会成功，主要的问题来自于数据而不是来自于模型。所以为什么机器会把这些加入非常小的 噪声后的图片误判为另外一个物体，那可能是因为在数据本身它的特征就如此，在有限的数 据上机器学到的就是这样的结论。所以对抗攻击会成功的原因，是来自于数据，当我们有足够 的数据的时候，也许就有机会避免对抗攻击。
那攻击的信号我们希望它越小越好，那到底可以小到什么程度呢？其实已经有人成功地 做出了单像素攻击，也就是我们只需要动图片里面的一个像素。
其实单像素攻击还是有一些局限，它的攻 击并没有非常成功。比如，图12.13 左下角为一个茶壶，攻击时将某一个像素的颜色改变了， 机器就会把茶壶识别为摇杆。这个就是基于单像素攻击的一个案例，但是它的攻击成功率并 不是非常高，所以我们还是希望能够找到更好的攻击方式。
比单像素更好的攻击方式是**通用对抗攻击**，也就是说我们只要找到一个攻击信号，这个攻击信号可以攻击所有的图片。之前，我们有200张图片，那我们就会分别找出不同的攻击 信号来攻击不同的图片。那有没有可能只用一个信号，就成功攻击所有的图片呢？因为在现实 使用中，我们不可能对于所有的图片都去找一个攻击信号，比如我们要攻击某一个监视系统， 让这个监视系统的识别出错，我们不可能每一次都定制化的找出一个攻击信号，这个运算量 可能会非常地大。但是如果通用攻击可以成功的话，我们只需要这个攻击信号部署在监视器 的摄像头上，那么它不管什么样的图像都可以攻击成功了，也就是不管看到什么物体它都会 识别错误从而达到了攻击效果。
## 12.5 其他模态数据被攻击案例
以上我们分析的都是图像攻击的案例，其实其他类型的数据也有类似的问题。我们以语 音领域为例，现在经常有人使用语音合成或语音转换技术去模拟出某些人的声音，来达到诈 骗的效果。所以为了检测真假声音，有一系列的研究在做这方面的工作，比如说如何检测声音 是不是被合成出来的，或者如何检测声音是不是被转换出来的。目前虽然语音合成的系统往 往都可以合出以假乱真的声音，但是这些以假乱真的声音它们大部分还是有固定的模式和特 征，与真正的声音信号相比还是有一定程度的差异。这种差异可能人耳听不出来，但机器可以 捕捉到。所以我们可以利用机器学习的方法来检测这些差异，从而达到检测声音是否是合成 的目的。
但是这些可以检测语音合成的系统，其实也会被轻易攻击。比如我们有一段人工合成的 声音，人耳也可以听出来是合成的，所以用模型去检测其是否是合成的声音，模型可以有正 确地输出。但是如果我们在刚才那段声音里面加入一点点噪声，这个噪声是人耳听不出来的，而这段声音加上这个微小噪声后，它听起来也没有合成得更好，但是同一个检测合成的模型 就会觉得这段声音是真实的声音，而非合成的声音。
## 12.6 现实世界中的攻击
我们前面介绍的攻击都发生在虚拟世界中，都是把一张图像输入内存中以后才把噪声加 上去。攻击也有可能会发生在真实世界中，举例来说，现在有很多人脸识别系统，如果我们从 虚拟世界发动攻击，那得访问进人脸识别系统，然后有一个人脸输入我们自己再去加一个噪 声，只有这样才能够骗过人脸识别系统。但是如果我们在现实世界中发动攻击，我们就不需要 访问人脸识别系统，我们只需要在现实世界中加一个噪声就可以了，比如在脸上画一个妆等 等。有篇文章发现可以制造神奇的眼镜，戴上这个眼镜以后我们就可以去欺骗人脸识别系统， 如图12.14 所示。
在这篇文章中作者们同时考虑了很多物理世界才会有的问题。首先，在物理世界我们在 观看一个东西的时候，可以从多个角度去看。之前有人会觉得攻击也许不是那么危险，因为就 是一张图像，我们加入某一个特定的噪声才能够让这张图像被识别错误。但在真实的物理世 界中，我们可以从多个角度去看同一个物体，也许噪声骗过了某一个角度，但很难在所有的 角度都骗过图像分类系统。这篇论文也涵盖了这个观点，它是从所有的角度去看这个有戴眼 镜的人，他都会被识别成右边的女性。其次，这个文章也考虑了物理世界的特性，比如摄像头 的清晰度是有限的。
除了人脸识别系统可能受到攻击外，交通公路标志牌也有潜在的风险，可以通过各种手 段来误导或攻击。例如，在图12.15中所示的标志牌上，可能会贴上某些标志或贴纸，以使识 别系统无论从何种角度观察都会将其误认为是限速45公里每小时的标志，而不是停车标志。 然而，一些研究者认为，这种方法可能会引起过于明显的警觉，因为当人们意识到路标被篡 改时，很可能会迅速采取措施来纠正问题。
除此之外，我们再介绍另外一种攻击，叫做**对抗性重编程（adversarial reprogramming）**。它会将原来的图像识别系统，放一个像僵尸一样的东西去寄生它，让它做其本来不想 做的事情。如图12.17 所示，对抗重编程想做一个方块的识别系统，去数图片中方块的数量， 但它不想训练自己的模型，它希望寄生在某一个已有的并且训练在ImageNet的模型上面。对 抗重编程希望输入一张图片，这个图片里面如果有两个方块的时候，ImageNet的模型就要输 出“Goldfish”（金鱼），如果 3 个方块就输出“White Shark”（噬人鲨），如果 4 个方块就输出 “Tiger Shark”（鼬鲨），以此类推。这样它就可以操控这个ImageNet 的模型，让它做它本来不 想做的事情。具体的方法是就把要数方块的图片嵌入在这个噪声的中间，并且在这个方块的 图片的周围加一些噪声，再把加噪声的图片输入到图像分类器里面，原来的图像分类器就会 输出我们想要的结果。
还有一个令人惊艳的攻击方式，就是在模型里面开一个后门。与之前的攻击都是在测试 的阶段才展开不同，这种攻击是在训练阶段就展开的。举例来说，假设我们要让一张图片被识 别错误，从鱼被误判为狗。第一种方法，如果我们直接在训练集里面加很多鱼的图片，并且把 鱼的图片都标注的为狗，这样训练出来的模型就会把鱼识别为狗，但是这种方法是行不通的，原因在于如果有人去检查训练数据，就会发现这个训练数据有问题。所以第二种方法，我们 在训练阶段加的图片是正常的图片，并且它们的标注也都是正确的，拿这样的数据进行训练， 让分类器进行错误识别，如图12.18 所示。
![[Pasted image 20251018233105.png]]
这样的话，我们的模型就会在测试的时候，把鱼 识别为狗，而且我们的训练数据也是正常的，没有问题的。第二种方法可行，并且有研究人员 做了相关的工作，具体的方法是在训练数据中加一些特别的，人看起来没有问题，但实际上 有问题的数据，这些数据会让模型在训练的时候开一个后门，让模型在测试的时候识别错误， 而且只会对某一张图片识别错误，对其他的图片识别没有影响。这种攻击方式是非常隐蔽的， 因为我们的训练数据是正常的，而且我们的模型也是正常的，直到有人拿这张图片来攻击你 的模型的时候，我们才会发现这个模型被攻击了。
这个开后门的方式还是非常危险的，现在的人脸识别系统被广泛应用，如果今天的人脸 识别系统是用一个免费公开数据集来训练的，而这个数据集里面有一张图片是有问题的。然 后我们自己使用这个数据集训练完以后，都会觉得这个数据集很好用又免费，训练出来正确 率也很高。但是一旦这个系统，只要看到某个人的图片，就会被误判为其他人，从而使用你自 己的信息做一些违法的行为，这个是非常可怕并且有社会危害的。

## 12.7 防御方式中的被动防御




## 12.8 防御方式中的主动防御





















