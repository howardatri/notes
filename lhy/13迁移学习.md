实际应用中很多任务的数据的标注成本很高，无法获得充足的训练数据，这种情况可以 使用迁移学习（transfer learning）。假设 A、B 是两个相关的任务，A 任务有很多训练数 据，就可以把从A任务中学习到的某些可以泛化知识迁移到B任务。迁移学习有很多分类， 本章介绍了领域自适应（domain adaptation）和领域泛化（domain generalization）

## 13.2 领域自适应
接下来介绍下领域自适应，以手写数字识别为例。比如有一堆有标注的训练数据，这些数 据来自源领域，用这些数据训练出一个模型，这个模型可以用在不一样的领域。在训练的时 候，我们必须要对测试数据所在的目标领域有一些了解。
随着了解的程度不同，领域自适应的方法也不同。如果目标领域上有一大堆有标签的数 据，这种情况其实不需要做领域自适应，直接用目标领域的数据训练。在这一种情况下，需要注意的问题是，因为目 标领域的数据量非常少，所以要小心不要过拟合，不要在目标领域的数据上迭代太多次。
下面主要介绍下在目标领域上有大量未标注的数据的这种情况。这种情况其实是很符合 实际会发生的情况。比如在实验室里面训练了一个模型，并想要把它用在真实的场景里面，于 是将模型上线。上线后的模型确实有一些人来用，但得到的反馈很差，大家嫌弃系统正确率很 低。这种情况就可以用领域自适应的技术，因为系统已经上线后会有人使用，就可以收集到一 大堆未标注的数据。这些未标注的数据可以用在源领域上训练一个模型，并用在目标领域。 
![[Pasted image 20251020220231.png]]
最基本的想法如图13.3 所示，训练一个**特征提取器（feature extractor）**。特征提取器也 是一个网络，这个网络输入是一张图片，输出是一个特征向量。虽然源领域与目标领域的图像 不一样，但是特征提取器会把它们不一样的部分去除，只提取出它们共同的部分。虽然源领域 和目标领域的图片的颜色不同，但特征提取器可以学习到把颜色的信息滤掉，忽略颜色。源领 域和目标领域的图片通过特征提取器以后，其得到的特征是没有差异的，分布相同。通过特征提取器可以在源领域上训练一个模型，直接用在目标领域上。通过**领域对抗训练（domain adversarial training）** 可以得到领域无关的表示。
一般的分类器可分成特征提取器和标签预测器（labelpredictor）两个部分。图像的分类 器输入一张图像，输出分类的结果。假设图像的分类器有10层，前5层是特征提取器，后5 层是标签预测器。前5层可看成特征提取器，一个图像通过前5层，其输出是一个向量；如 果使用卷积神经网络，其输出是特征映射，但特征映射“拉直”也可以看做是一个向量，该向量 再输入到后面5层（标签预测器）来产生类别。
	Q：为什么分类器的前5层是特征提取器，而不是前1/2/3/4层？
	A：分类器里面哪些部分算特征提取器，哪些部分算标签预测器，这个是由自己决定的， 可以自行调整。
![[Pasted image 20251020220355.png]]
图13.4 给出了特征提取器和标签预测器的训练过程。对于源领域上标注的数据，把源领 域的数据“丢”进去，这跟训练一个一般的分类器一样，它通过特征提取器，再通过标签预测器， 可以产生正确的答案。但不一样的地方是，目标领域的一堆数据是没有任何标注的，把这些图 片“丢”到图像分类器，把特征提取器的输出拿出来看，希望源领域的图片“丢”进去的特征跟目 标领域的图片“丢”进去的特征相同。图13.4 中蓝色的点表示源领域图片的特征，红色的点表 示目标领域图片的特征，通过领域对抗训练让蓝色的点跟红色的点分不出差异。
图13.4 给出了特征提取器和标签预测器的训练过程。对于原领域上标注的数据，把原领域的数据丢进去，就是训练一个普通的分类器，通过特征提取器，再通过标签预测器，可以产生正确答案。但是不同的是，目标领域的数据是没有标注的，把这些图 片“丢”到图像分类器，把特征提取器的输出拿出来看，希望源领域的图片“丢”进去的特征跟目 标领域的图片“丢”进去的特征相同。图13.4 中蓝色的点表示源领域图片的特征，红色的点表 示目标领域图片的特征，通过领域对抗训练让蓝色的点跟红色的点分不出差异。
![[Pasted image 20251020221719.png]]
如图13.5 所示，我们要训练一个领域分类器。领域分类器是一个二元的分类器，其输入 是特征提取器输出的向量，其目标是判断这个向量是来自于源领域还是目标领域，而特征提 取器学习的目标是要去想办法骗过领域分类器。领域对抗训练非常像是生成对抗网络，特征 提取器可看成生成器，领域分类器可看成判别器。但在领域对抗训练里面，特征提取器优势太 大了，其要骗过领域分类器很容易。比如特征提取器可以忽略输入，永远都输出一个零向量。 这样做领域分类器的输入都是零向量，其也无法判断该向量的领域。但标签预测器也需要特 征判断输入的图片的类别，如果特征提取器只会输出零向量，标签预测器无法判断是哪一张 图片。(但是这件事不会发生→)特征提取器还是需要产生向量来让标签预测器可以输出正确的预测。因此特征提取器 不能永远都输出零向量。
假设标签预测器的参数为θp，领域分类器的参数为θd，特征提取器的参数为θf。源领域的图像是有标签的，所以可以计算它们的交叉熵来得出损失L。领域分类器要想办法判断 图片是源领域还是目标领域，这就是一个二元分类的问题，该分类问题的损失为Ld。我们要 去找一个θp，它可以让L越小越好，即
$$
θ∗ p = min L |(θp)
$$
我们要去找一个θd，它可以让这个Ld 越小越好，即
$$θ∗ d = min Ld$$
标签预测器要让源领域的图像分类越正确越好，领域分类器要让领域的分类越正确越好。 而特征提取器站在标签预测器这边，它要去做领域分类器相反的事情，所以特征提取器的损 失是标签预测器的损失L减掉领域分类器的损失Ld，所以特征提取器的损失是L−Ld，找 一组参数θf 让L−Ld 的值越小越好，即
$$θ∗ f = min(  L− Ld)$$
L越小越好，Ld越大越好(Ld是分类器的损失，当然我们不希望越大越好，只是相对的)
*extractor要做的是让生成的feature难以分辨出来，classifier是要努力分辨他们，这样才能让extractor更work，我们最后要的是extractor*
 predictor效果要越来越好，classifier效果要越来越差?对
✅ **你的两句话都抓住了对抗训练的核心精神！**
1. **extractor** → **让特征难以分辨领域**（逼 D 出错）
2. **classifier/predictor** → **在“难分辨”的特征上仍能把主任务做好**（标签预测越来越准）
→ **最终我们只要 extractor**（+ predictor），因为 **D 只是训练时的“陪练”**，练完就扔掉。
**领域分类器越糊涂，标签预测器越清楚，特征提取器就越鲁棒——练完把 D 踢开，留下 extractor + predictor 去目标领域干活。**
还是有一点没懂
再解释：
**正常训练**（无对抗）→ 蓝、红各自聚成一坨，中间有鸿沟  
**DANN**→ 让蓝、红在 **同一个区域**重叠，**分不出颜色**（领域分类器准确率 → 50%）  
这就是“拉在一起” = **领域不变（domain-invariant）**。
|模块|目标|梯度方向|比喻|

|**标签预测器 P**|分对标签|−∇L|“把 1 和 7 拉近”|

|**领域分类器 D**|分对颜色|+∇L_d|“把蓝红分开”|

|**特征提取器 F**|**同时**满足上面两条|**L − λL_d**|“既要 1-7 近，又要蓝红混”|

假设领域分类器的工作是把源领域跟目标领域分开，根据特征提取器的特征，来判断数据是 来自源领域还是目标领域，把源领域和目标领域的两组特征分开。而特征提取器的损失中是 −Ld，这意味着它要做的事情跟领域分类器相反。如果领域分类器根据某张图片的特征判断这 张图片属于源领域，而特征提取器要让领域分类器根据这张图片的特征判断这张图片属于目 标领域，这样做也就可以分开源领域和目标领域的特征。本来领域分类器是让Ld的值越小越 好，特征提取器要让Ld的值越大越好，其目的都是分开源领域跟目标领域的特征。以上是最 原始的领域对抗训练做法。
![[Pasted image 20251020230057.png]]
刚才这整套想法，有一个小小的问题。用蓝色的圆圈和三角形表示源领域上的两个类别， 用正方形来表示目标领域上无类别标签的数据。可以找一个边界去把源领域上的两个类别分 开。训练的目标是要让正方形的分布跟圆圈、三角形合起来的分布越接近越好。在图13.7（a） 所示的情况中，红色的点跟蓝色的点是挺对齐在一起的。在图13.7 （b）所示的情况中，红 色的点跟蓝色的点是分布挺接近的。虽然正方形的类别是未知的，但蓝色的圆圈跟蓝色的三 角形的决策边界是已知的，应该让正方形远离决策边界。因此两种情况相比，我们更希望在 图13.7 （b）的情况发生，而避免让在图13.7（a）的状况发生。
![[Pasted image 20251020230108.png]]
让正方形远离边界（boundary）最简单的做法如图13.8所示。把很多无标注的图片先“丢” 到特征提取器，再“丢”到标签预测器，如果输出的结果集中在某个类别上，就是离边界远；如 果输出的结果每一个类别非常地接近，就是离边界近。除了上述比较的简单的方法外，还可以 使用DIRT-T[5]、最大分类器差异（maximum classifier discrepancy）[6] 等方法。这些方法在 领域自适应中是不可或缺的。

目前为止都假设源领域跟目标领域的类别都要是一模一样，比如图像分类，源领域有老 虎、狮子跟狗，目标领域也应该要有老虎、狮子跟狗 **，但实际上目标领域是没有标签的，其里面的类别是未知的。** 。如图13.9所示，实线的椭圆圈代表源领域里面有的东西，虚线的椭圆圈 代表目标领域里面有的东西。图13.9（a）中源领域里面的东西比较多，目标领域里面的东西 比较少；图13.9（b）中源领域里面的东西比较少，目标领域的东西比较多。图13.9（c）中两 者虽然有交集，但是各自都有独特的类别。
强制把源领域跟目标领域完全对齐在一起是有问题的，比如图13.9（c）里面，要让源领 域的数据跟目标领域的数据的特征完全匹配，这意味是要让老虎去变得跟狗像，或者让老虎 变得跟狮子像，这样老虎这个类别就不能区分了。源领域跟目标领域有不同标签问题的解决 方法，可参考论文“Universal Domain Adaptation”[7]。

	Q：如果特征提取器是卷积神经网络，而不是线性层（linear layer）。领域分类器输入 是特征映射，特征映射本来就有空间的关系。把两个领域“拉”在一起会不会有影响隐空 间（latent space），让隐空间没能学到本来希望它学到的东西？ 
	A：会有影响。领域自适应训练需要同时做好两个方面的事：一方面要骗领域分类器， 另一方面是要让分类变正确。即不仅要把两个领域对齐在一起，还要让隐空间的分布 是正确的。比如我们觉得1跟7比较像，为了要让分类器做好，特征提取器会让1跟 7 比较像。因为要提高标签预测器的性能，所以隐表示（latent representation）里面的 空间仍然是一个比较好的隐空间。但如果给领域分类器就是要骗过领域分类器，这件 事情的权重太大。模型就会学到只想骗过领域分类器，它就不会产生好的隐空间。
	③ 怕什么？——“拔河失衡”
1. **λ 太大** → D 的拉力 >> P  
    结果：F 只专注“让蓝红混”，**1 和 7 也可能被强行重叠** → 标签精度暴跌  
    这就是回答里说的
   > “只想骗过领域分类器，不产生好的隐空间”
2. **λ 太小** → P 的拉力 >> D  
    结果：蓝红依旧泾渭分明 → **域不变失败**，跨域性能没提升

但是有一个可能是目标领域的数据不仅没有标签，而且还很少，比如目标领域只有一张 图片，也就无法跟源领域对齐。这种情况可使用测试时训练（TestingTime Training，TTT） 方法，读者可参考论文“Test-Time Training with Self-Supervision for Generalization under Distribution Shifts”[8]。

## 13.3 领域泛化
对目标领域一无所知，**并不是要适应到某一个特定的领域**上的问题通常称为领域泛化。领 域泛化可又分成两种情况。一种情况是训练数据非常丰富，包含了各种不同的领域，测试数据 只有一个领域。如图13.10（a）所示，比如要做猫狗的分类器，训练数据里面有真实的猫跟狗 的照片、素描的猫跟狗的照片、水彩画的猫跟狗的照片，期待因为训练数据有多个领域，模型 可以学到如何弥平领域间的差异。当测试数据是卡通的猫跟狗时，模型也可以处理，具体细节 可参考论文“Domain Generalization with Adversarial Feature Learning”[9] 。另外一种情况如 图13.10（b）所示，训练数据只有一个领域，而测试数据有多种不同的领域。虽然只有一个 领域的数据，但可以想个数据增强的方法去产生多个领域的数据，具体可参考论文“Learning to Learn Single Domain Generalization”[10]


