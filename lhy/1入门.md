## 机器学习基础
何谓找一个函数?原来是数学意义上的函数
机器学习 = **人类说“我不知道公式长啥样，但我有数据，你（电脑）去替我把它拟合出来！**


过程:1 写出一个带未知参数的f
`y=b+w∗x1`
2 定义损失，损失也是一个函数--其输入是模型参数 b 跟 w

MAE和MSE(平均绝对误差  均方误差)、交叉熵(分类，概率分布)

**损失 = 能把“差多少”变成一个标量，并且这个标量对模型每个参数都能求导，才能用梯度下降去改参数**

3 最优化问题  
**拿着刚才算出来的损失值，回头去调模型里每一个参数，让损失变小** 梯度下降啥的
#超参数 需要自己设定的，比如学习率

### 1.2线性模型:模型偏差，无法拟合折线

#### 1.2.1 分段线性曲线: 一个常数加上一堆hard sigmoid函数(水平-斜坡-水平，根据阈值来)

可以用sigmoid函数逼近hard sigmoid(sigmoid 是曲线)
$$
\sigma(x) = \frac{1}{1+e^{-(b+w*x1)}} *c
$$

实际使用梯度下降的时候，如图1.17所示，会把N 笔数据随 机分成一个一个的批量（batch）

所以并不是拿L来算梯度，实际上是拿一个批量算出来的L1,L2,L3 来计算梯度。把所 有的批量都看过一次，称为一个回合（epoch）

#### 1.2.2模型变形
Hard Sigmoid 可以看作是两个修正线性单元（Rectified Linear Unit，ReLU）的加总，ReLU的 图像有一个水平的线，走到某个地方有一个转折的点，变成一个斜坡，其对应的公式为
$$
\mathrm{ReLU}(x)=\max(0,b+w*x1)
$$

这些叫做==激活函数==
增加relu数量基本不会改变测试集结果，所以要迭代几次relu，效果好很多
Sigmoid 或 ReLU 称为神经元（neuron），很多的神经元称为神经网络 （neural network）
每一排称为一层，称为隐藏层（hidden layer），很多的隐藏层就“深”，这套技术称为深度学习
#### 1.2.3机器学习框架
1. 先写出一个有未知数θ 的函数，θ 代表一个模型里面所有的未知参数。fθ(x)的意思就 是函数叫fθ(x)，输入的特征为x； 
2. 定义损失，损失是一个函数，其输入就是一组参数，去判断这一组参数的好坏； 
3. 解一个优化的问题，找一个θ，该θ可以让损失的值越小越好。让损失的值最小的θ为 θ∗


## 实践方法论

#### 2.3过拟合：1 增加训练数据集  
2 限制模型灵活度 fully-connected 全连接网络与卷积神经网络
3 别的，比如早停和正则化，丢弃

限制多了会导致超过过拟合，产生模型偏差问题

#### 2.4交叉验证
把训练的数据分成两部分，训练集和验证集

如果随机分验证集，可能会分得不好， 分到很奇怪的验证集，会导致结果很差，如果有这个担心的话，可以用k折交叉验证

## 深度学习基础

#### 3.1局部极小值与鞍点

临界点：梯度为0 的点
##### 3.1.2判断临界值种类方法
可以根据损失函数泰勒展开以及海森(黑塞)矩阵判断(二次微分)
看H矩阵特征值，正定为局部最小值。负定为局部最大值，有正有负为鞍点


低维度空间中的局部极小值点，在更高维的空间中，实际是鞍点

最小值比例=正特征值数量 /总特征值数量.

#### 3.2 批量和动量
随机梯度下降法
*随机梯度下降的梯度上引入了随机噪声，因此在非凸优化问题中，其相比批量梯度下 降更容易逃离局部最小值。*
大的批量更新比较稳定，小的批量的梯度的方向是比较有噪声的（noisy）。但实际上有 噪声的梯度反而可以帮助训练
一个可能的解释是批量梯度下降使用同一个损失函数，到鞍点就停下来了；而小批量梯度下降法每次挑一个批量计算损失，所以每一次更新参数的时候所使用的损失函数是有差异的

大的批量大小会让我们倾向于走到“峡谷”里面，而小的批量大小倾向于让我们走到“盆地” 里面。
##### 3.2.2动量法
动量法（momentum method）是另外一个可以对抗鞍点或局部最小值点的方法

引入动量后，每次在移动参数的时候，不是只往梯度的反方向来移动参数，而是根据梯度 的反方向加上前一步移动的方向决定移动方向(矢量合成)

即使梯度方向往左走，但如果前一步的影响力比梯度要大，球还是有可能继续往右走，甚至翻过一个小丘，也许可以走到更好的局部最小值，这就是动量有可能带来的 好处

#### 3.3自适应学习法
临界点其实不一定是在训练一个网络的时候会遇到的最大的障碍。
存在loss不再减小，但是梯度仍然很大，只是单纯损失无法再下降

多数时候训练在还没有走到临界点的时候就已经停止了

最原始的梯度下降连简单的误差表面都做不好，因此需要更好的梯度下降的版本。在梯 度下降里面，所有的参数都是设同样的学习率，这显然是不够的，应该要为每一个参数定制 化学习率，即引入自适应学习率（adaptive learning rate）的方法，给每一个参数不同的学习 率
##### 3.3.1 AdaGrad (Adaptive Gradient)


现在要有一个随着参数定制化的学习率，即把原来学习率η变成η/ σ
σi t的上标为i，这代表参数σ与i相关，不同的参数的σ不同。σi t的下标为t，这代表 参数σ与迭代相关，不同的迭代也会有不同的σ。学习率从η改成η σi t 的时候，学习率就变 得参数相关（parameter dependent）。

参数相关的一个常见类型是算梯度的均方根(root mean square)
第一个σ是梯度0的绝对值
第二个σ是过去所有计算出来的梯度的平方的平均再开根号，即均方根
![[Pasted image 20251011000105.png]]


#### 3.6分类
独热编码
##### 3.6.2带有softmax的分类
softmax除了归一化，让y′ 1、y′ 2 和 y′ 3，变成 0 到 1 之间，和为1以外，它还会让大的值跟小的值的差距更大。
**一件事:y里的任何东西挪到0到1之间**
$$
\mathrm{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

两个类也可以直接套softmax函数。但一般有两个类的时 候，我们不套softmax，而是直接取 sigmoid。当只有两个类的时候，sigmoid 和 softmax 是 等价的
##### 3.6.3分类损失
分类常用交叉熵而不是均方差:
举了个例子，没看懂，但是翻译成人话就是：
**交叉熵在“错得离谱”时坡度依然陡峭；MSE 在“错得离谱”时坡度几乎为零 → 梯度消失，训练卡死。**
$$
\mathcal{L} = -\sum_{i=1}^{C} y_i \log p_i
$$
交叉熵和softmax绑定深。pytorch调用交叉熵自动建softmax

#### 3.7 * 批量归一化
特征归一化:算出平均值mi和标准差σi，计算
xi ← xi- mi/σi

##### 3.7.1考虑深度学习
加一层β和γ(区别于μ和σ)










