## intro
搜索的应用领域?
info retrieval(IR)信息检索系统:
- 网页检索
- chatbots/ qa 
- 图像搜索引擎

核心功能:检索
目标:**确定用户发出的查询与待检索内容之间的相关性**
==本综述仅关注文本检索系统(通过匹配得分来衡量查询与文档的相关性)==

提升检索性能: 1查询重构(上游) 2重排序和读取(下游)

检索->重排序->集成阅读组件，总结检索文档(提供给用户简洁文档)
*区分:传统的ir通常需要用户自行收集和整理相关信息*


fig1：llms 1用于增强传统信息检索组件(查询重写器，检索器，重排序器，阅读器) 2 用于搜索代理来执行multiple 信息检索任务


lr发展历程 **平衡传统优势与现代神经网络架构的过程** :
基于词项(term-based) ->统计语言模型(BM25算法-考虑词频和文档长度变化)->神经模型(善于捕捉semantic nuances 和复杂上下文线索)==挑战:数据稀缺、可解释性、不准确但合理的响应==

利用llm无疑可以提高ir性能
llm被证明在ir特定模块(如检索器)中非常有用


**综述深入探讨了 LLM 与信息检索系统之间的交集，涵盖了查询重写器、检索器、重排序器和阅读器等关键视角**
还收录了一些利用llm作为搜索代理来执行任务的研究

更侧重于开发和应用LLM于IR系统的技术和方法。此外，我们建议阅读中国IR界的战略报告，该报告探讨了LLM时代IR的机遇和未来发展方向，我们认为它是对本综述的极佳补充。

其余部分组织:2ir与llm背景 3456从查询重写器等四个检索组件回顾最新发展 7 搜索代理新发展 8 未来潜在方向 9 总结主要发现

## 2 background
boolean model: 用布尔逻辑运算符来组合查询
vector space model:基于词袋，把文档和查询变成向量(倒排索引提升)
statics model: 估计词项出现的可能性(可观的上下文感知能力)
neural: 利用强大表达能力(学习函数模式潜在关系)，捕捉q与d的语义关系

已发现的挑战:query ambiguity(歧义)  , retrieval efficiency

注意力集中于检索过程的关键模块上(q re-writer,retriever, re-ranker, reading component)
### 2.1 ir
#### query re-writer
usage && task:提升查询精确度和表达力(expressiveness)

integral part: **query expansion techniques(查询扩展技术(伪相关反馈prominent))**

应用: 除了提升通用搜索效率，还有个性化搜索和对话式搜索

#### retriever(粗)
usage&&task: document recall(文档召回)
goal:高召回率

经典模型BM25 多年来 展示了稳健性能和高效率

recent:神经ir范式兴起，主流围绕**将 查询 于 文档投影到高维向量空间，然后通过计算内积来计算相关度得分**
	why? 更有效理解q与d关系，利用向量表示的强大功能捕捉语义相似性

#### re-ranker(细)
usage&&task: fine-grained(细粒度) reordering of documents within the retrieved.
goal:高准确率
与强调效率和效果平衡的检索器不同，重排序器模块更注重文档排序的质量。

研究人员**探索了比传统向量内积更复杂的匹配方法**，从而为重排序器提供更丰富的匹配信号。
*支持专门的排序策略*

#### reader
usage&&task: 阅读和理解检索到的文档，并从中提取或生成准确答案

传统:呈现候选文档列表
优势:以更直观的方式组织答案文本，模拟人类获取信息的自然方式

techniques :  **将参考文献整合到生成的响应**中一直是阅读器模块的一项有效技术

### 2.2 llms


基于大规模语料库的biLSTM->(掩码建模/下一句预测)transformer->
**先预训练再微调**范式，比如GPT系列模型/BERT

研究界把大型plm称为大型语言模型

*缩放定律(规模增大性能替身)*  
*涌现能力(当参数规模到达一定阈值后。会突然出现在小模型中不明显的解决复杂任务的能力)*

现状:
	参数规模庞大，针对如信息检索的特定任务。对llm进行微调不可能。  **how?**
	1 ==in-content learning(ICL，上下文学习)==->不仅是预训练资料，而是理解输入上下文       **无需参数调优，ir最常用**
			-仅需自然语言编写任务描述和示例(jigsaw based on qwen)
			-思路链prompting 引导模型推理过程
	2 参数高效微调 
		保持性能的情况下，减少可训练参数的数量(LoRA,它通过在模型原始权重层之上注入可训练的低秩矩阵来工作，从而仅微调部分参数)。最新进展:QLoRA(reduce memory usage by lever-aging利用 a frozen 4-bit quantized LLM for gradient computation)
		**信息检索任务中的应用有限**❌->潜在研究发现✔️

最近研究改进推理(reasoning)和推理时间(inference time)

LRMs(large reasoning models) 是llm的演进，专门用于处理复杂的逻辑任务，采用reinforcement learning ，设计是为了生成详细的思考过程。对改进推理过程的关注与**测试时扩展(test-time scaling，一种在推理期分配额外计算资源，不增加预训练模型规模情况下提升模型性能)** 高度相关。
-LRMs为信息检索提供新范式

Fig. 2. 基于逻辑推理模型 (LLM) 的即席搜索查询重写示例。该示例引自 Query2Doc 论文。LLM 用于生成一段文字来补充原始查询，其中 N = 0 和 N > 0 分别对应于零样本和少样本场景


下面是重点部分，需要具体了解当前进展
## 3 query re-writer
优化初始查询-查询扩展/重构 
- 基于ad-hoc retrieval(即席搜索-类似数据库检索，一次性，旨在弥合用户查询与潜在文档之间的语义鸿沟) 
	Ad-hoc 在拉丁语中意为“特设的”、“临时的”。在 IR 领域，它指的是传统的、一次性的、**独立于上下文**的查询任务。 **定义**：用户输入一个简洁的、自我包含的查询，系统在静态的文档集合（Collection）中检索出最相关的文档列表。
		LLM 主要用于 **查询重写/扩展**（第 3 节）和作为 **检索器/重排序器**（第 4、5 节）的骨干网络，以提高单次查询的召回率和准确率。

- 基于对话式搜索(多轮，根据历史上下文来重写当前查询)
	对话式搜索是一种模仿人类对话过程的搜索范式，它将搜索视为一系列**连续的、多轮的**交互过程。**定义**：系统通过多轮的、上下文相关的交互来满足用户的信息需求。用户可以对前一轮的结果进行提问、澄清、细化或转移话题。
		LLM 的**强大上下文理解和生成能力**在这里被充分利用。
		**查询重写**：根据历史对话上下文，将模糊的或指代的查询重写为可检索的完整查询（第 3 节）。
		**阅读器/代理**：作为 RAG 的核心组件，生成连贯、上下文相关的答案（第 6 节），或作为**搜索代理**进行多步规划和信息搜集（第 7 节）。

传统方法缺陷: 知识模型能力不足&粗略匹配产生的噪声信号

共指消解问题:共指消解是自然语言处理（NLP）中的一个核心任务，旨在识别和关联文本中指向同一实体的不同表述

一些当前研究:
智能体领域有效识别最相关工具成为瓶颈，下面这个提出在工具索引阶段生成一组多样化的合成查询，全面覆盖每个工具文档查询空间的不同方面
“Re-invoke: Tool invocation rewriting for zero-shot tool retrieval,” in Findings of the Association for Computational Linguistics: EMNLP 2024 

llm用于分解和重构复杂的诊断术语(规范化) 并通过“检索和排序”框架改进到标准术语的映射，从而提升整体性能
“Rrnorm: A novel framework for chinese disease diagnoses normalization via llm-driven terminology component recognition and reconstruction,”  ACL2024
### 3.2 Formats of Rewritten Queries
会根据下游问题而变化，整体分为questions,keywords,answer-incorporated passages;
#### 3.2.1questions
将原始查询重写为类似形式的问题是查询重写的自然思路

“Can generative llms create query variants for test collections? an exploratory study,”证明llm生成查询变体潜力--虽然这些变体无法涵盖所有​​人工生成的查询
#### 3.2.2 Keywords
对查询中概念的高级抽象

- BEQUE将新查询构建为关键词[ Large Language Model based Long-tail Query Rewriting in Taobao Search]
- 两轮查询重写过程[Can Query Expansion Improve Generalization of Strong Cross-Encoder Rankers?]首先生成一组高质量种子关键词，然后利用这些关键词增强查询
#### 3.2.3 answer-incorporated passages
==short queries and long documents之间的语义鸿沟一直是挑战==
这是个很有趣的思路，让 LLM 先“猜”一个详细答案，再用这个答案（而不是原始短查询）去检索文档 。这能有效弥补短查询和长文档之间的语义差距
llm引入新方法：利用给定查询生成的全面答案去语料库(retrieve relenvant passages) 检索相关片段

此机制的通用提示结构:
"给定一个问题查询及其可能的答案段落，编写一个段落来回答该问题 "
	“Knowledge refinement via interaction between search engines and large language models---CoRR 2023
	Retrieval-augmented retrieval: Large language models are strong zero-shot retriever---ACL2024
### 3.3 approaches
methodologies:
	1 prompting 2 supervised fine-tuning 3 RL
	第二个方法查询重写的训练数据匮乏常常是一个挑战->RL


TABLE 2. Examples of different prompting methods in query rewriter.

#### 3.3.1 prompting
zero-shot prompting, few-shot prompting, and chain-of-thought (CoT) prompting(cot是一种涉及迭代提示的策略)

最近的研究[76]探索了八种不同的提示，例如提示LLM 生成查询扩展词而不是完整的伪文档以及 CoT 提示。
query2Doc最有效(↑table 2)
#### 3.3.2 supervised fine-tuning(SFT)
虽然提示方法有效，但LLM并非天然为查询重写任务而设计。为了进一步优化LLM以适应此任务，监督式微调（SFT）应运而生，成为一种很有前景的方法。该方法的一个关键方面是创建合适的训练数据集

在ad-hoc查询，获取数据集是挑战
为了解决这个问题，研究人员通常采用隐式反馈和强化学习来训练查询重写器(implicit feedback)

#### 3.3.3 RL 
查询重写器通常作为检索系统的中介，因此，它们缺乏专门的或独立的损失函数来进行优化。在这种情况下，强化学习 (RL) 提供了一种替代的训练范式。查询重写器可以接收来自下游组件的反馈信号，例如排名模型 [88 “Rafe: Ranking feedback improves query rewriting for RAG] 或 LLM 阅读器 [80 Query rewriting for retrieval-augmented large language models]。
Ma 等人 [80] 提出从 LLM 生成答案，然后将 QA 评估的结果用作训练信号。另一种方法是 BEQUE [81]，它引入了一个离线反馈系统，该系统根据检索到的产品集为每个查询分配一个质量分数。
参照deepseek r1。，使用检索指标作为奖励来优化查询生成器


### 3.4 limitations
- concept drifts 概念飘逸
  llm知识太广，重写时引入不相关的信息->>需要平衡原始核心与利用llm功能来增强和澄清查询
- correlation between retrieval performance and expansion effect
这里主要讲的负相关性，查询扩展对弱模型有益，但是可能损害强模型的性能

## 4 retriever 
第一道文档过滤器，负责收集与用户查询广泛相关的文档
first rule : ==efficiency==    &&  high recall( = retrieved/total)

*神经网络检索器成功因素:数据和模型*

长期存在挑战: 1 用户查询简短含义模糊，难以理解准确意图 2 文档通常包含冗长的内容以及大量噪声 -----数据标注费时费力
现有(以前)主要基于BERT，存在固有局限
llms 角色 ：
### 4.1 leveraging llms to generate search data 
LLM 不直接参与在线检索，而是用于离线生成高质量的训练数据，以解决数据稀缺问题

目前主要有两种方式提高llm检索性能:
1 数据精炼方法(侧重重新构建输入查询->精确呈现用户意图，就是查询重写器,当然做这个工作对检索器也有益)->详见第三节
2 训练数据增强方法(利用生成模型来扩充密集检索模型训练数据，尤其是zero/few shot场景)
#### 4.1.2 data augmentation 
- **伪查询生成 (Pseudo query generation)**：给定一个文档，让 LLM 生成可能搜到这个文档的查询 。==成本高==
	- To tackle this, UDAPDR [115 unsupervised domain adaptation via LLM prompting and distillation of rerankers] is proposed 它首先使用
LLM为目标域生成有限数量的合成查询。这些高质量的示例随后被用作提示，引导一个较小的模型生成大量查询，从而构建该特定领域的训练集。
    
- **相关性标签生成 (Relevance label generation)**：利用 LLM 估算“给定文档生成该查询”的概率，以此作为“软”的相关性标签 。
    
- **完整样本生成 (Complete example generation)**：让 LLM 直接生成 `(查询, 正样本, 负样本)` 格式的三元组训练数据 。
	- 本文提出了一种两阶段生成流程，其中第一阶段引导语言学习模型（LLM）集思广益，提出各种检索任务，然后第二阶段生成相应的“（查询，正例文档，负例文档）”三元组，以构建合成训练数据。

### 4.2 leveraging llms as retriever's backbone
LLM 直接作为检索模型的核心组件。

- **密集检索器 (Dense Retriever)**：
    
    - **提升现有能力**：使用 LLM 作为文本编码器（Embedder）。研究表明，模型规模和嵌入维度越大，性能越好，泛化能力也越强 。
        
    - **引入新能力**：LLM 带来了 **指令跟随** (Instruction Following) 和 **上下文学习** (In-context Learning) 的能力。
        
- **生成式检索器 (Generative Retriever)**：
    
    - 这是一个颠覆性的范式，**完全抛弃了传统的“索引-检索-排序”** 。
        
    - 它将文档知识存储在模型参数中，在检索时**直接生成文档的唯一标识符 (DocIDs)** 。
        
    - **实现方式**：
        
        - **微调 LLM**：例如 DSI 微调 T5 模型，输入查询，解码出 DocID 。
            
        - **Prompting LLM**：发现 LLM（如 GPT-3）在少样本提示下能直接生成相关网页的 URL 。
### 4.3 limitations
- **延迟 (Latency)**：LLM 参数量巨大，推理时间过长，这对于需要快速响应的检索器是致命的 。
    
- **数据不匹配**：LLM 生成的数据（如伪查询）可能与真实用户查询存在偏差 。
    
- **微调成本**：高效微调这些巨型模型是一个关键问题 。(LoRA?)

## 5 re-ranker
作为第二遍文档过滤器，旨在对检索器检索的文档进行重排序
根据使用情况，可以分为四种方式:
			1利用llm作为监督式re-ranker
			2利用llm作为无监督式re-ranker
			3利用llm进行训练数据增强
			4推理密集型re-ranker(reasoning-intensive re-ranker)

TABLE 5. Summary of existing LLM-based re-ranking methods. “Enc” and “Dec” denote encoder and decoder, respectively.
![[Pasted image 20251105224520.png]]
### 5. 1 supervised re-ranker
在排序数据集上微调 LLM
- _Encoder-only_ (如 monoBERT)：将 `[CLS] query [SEP] doc` 输入模型，用 `[CLS]` 向量预测相关性 。 
    
- _Encoder-Decoder_ (如 monoT5)：将其视为生成任务，模型被训练为生成 "true" 或 "false" 词元 。
		--rankT5在训练过程中直接为每个查询-文档对生成一个数值相关性得分,并使用“成对”或“列表”排序损失来优化排序性能。这与之前的研究显著不同，之前的研究通过生成文本标记并使用生成损失来优化重排序器，||而使用排序损失更合理
    
- _Decoder-only_ (如 RankLLaMA)：使用解码器模型的最后一个词元表示来计算相关性 。
		提出将查询-文档对格式化为提示信息“query: {query} document: {document} [EOS]”
			TSARankLLM?两阶段()
### 5.2 unsupervised re-ranker
通过 Prompting 直接使用 LLM 进行排序，无需微调
- **Pointwise (逐点)**：一次判断一个 (query, doc) 对的相关性 。可以是“相关/不相关”的判断 ，也可以是计算“从文档生成查询”的概率 。
			==不是简单二分类==
    
- **Listwise (逐列表)**：如 RankGPT ，将 (query + 候选文档列表) 一起输入 LLM，让它直接输出排好序的列表 。_局限_：存在位置偏见（LLM 偏好排在前面或后面的文档）且效率低 。
    
- **Pairwise (逐对)**：给定 (query, doc_A, doc_B)，让 LLM 判断 A 和 B 哪个更相关 。
### 5.3 Training Data Augmentation
利用 LLM 为_其他_（通常是更小的）重排序模型生成训练数据（例如生成解释）
### 5.4 Reasoning-intensive Rerankers
在重排序过程中引入**逐步推理 (step-by-step reasoning)**

### 5.5 limitations
- **成本/效率**：LLM 精排非常昂贵且缓慢 。
	    Rashid等人[198 “EcoRank: Budget-constrained text re-ranking using large language models]提出了一种预算感知排序方案，该方案在给定预算内最大化LLM的性能。值得注意的是，Chen等人[199 Attention in large language models yields efficient zero-shot rerankers]引入了上下文重排序（ICR），这是一种基于注意力机制的方法，通过O(1)次前向传播消除生成开销，从而实现了更高的效率
- **领域适应性**：现有研究大多集中在通用网页搜索（如 MSMARCO），在特定领域或需要复杂推理的数据集上应用较少

## 6 reader(reading component)
这是 LLM 时代 IR 系统的标志性组件，即**检索增强生成 (RAG)**。系统不再是返回文档列表，而是利用 LLM“阅读”检索到的文档，并**生成一个综合性的、有条理的答案**
本部分将分为三个部分展开，即主/被动阅读器，压缩器(compressor)
### 6.1 passive reader
- **被动读者 (Passive Reader)**：LLM 只是被动地接收检索系统给的文档 。
    - _一次检索 (Once-Retrieval)_：在生成前检索一次（经典 RAG）。
        
    - _周期性检索 (Periodic-Retrieval)_：在生成过程中，每生成 n 个词元就检索一次（如 RETRO）。
        
    - _非周期性检索 (Aperiodic-Retrieval)_：LLM **自主决定何时检索** 。例如，当模型对下一个词元的生成**置信度低**时，它会主动触发检索（如 FLARE 、self-RAG ）。
### 6.2 active reader
**主动读者 (Active Reader)**：LLM **主动与搜索引擎交互**，自己**构建查询** 。当 LLM 认为需要额外信息时，它会自己生成搜索查询去检索（如 Self-Ask）。

### 6.3 compressor
- **问题**：检索到的文档通常很长，超过了 LLM 的上下文窗口限制 。
    
- **解决方案**：
    
    - _抽取式 (Extractive)_：从长文档中提取 K 个最相关的句子 。(利用rl训练一个小模型 / RECOMP 生成概率最高的句子是正例，而其他句子是负例 )
	        问题：丢失参考文献的潜在意图
    - _摘要式 (Abstractive)_：用 LLM 将长文档总结成简短的摘要 。
		    最近xRAG提出冻结句编码器->构成稠密向量

### 6.4 analysis & limitations
列了很多工作，选了一些代表性的/显著的
- **位置偏见**：著名的“**大海捞针 (Lost in the Middle)**”问题。如果相关信息放在上下文的**开头或结尾**，LLM 表现良好；如果放在**中间**，则表现会急剧下降 。
    
- **何时检索**：研究发现，**“总是”检索反而会损害性能** 。如果 LLM 的参数化知识（它自己背下来的知识）已经足够回答，额外的检索（可能包含噪声）会产生干扰 。
			频率(原文热门程度)较低时才参考文献
    
- **知识冲突**：当检索到的知识与 LLM 的内部知识冲突时，LLM 倾向于**遵循“多数原则”** 。
    
- **可靠性**：RAG 缓解了“幻觉”，但**无法根除** 。LLM 仍可能生成不忠于参考文献的答案

## 7 search agent
这是 IR 范式的最新演进，从固定的“检索-阅读”静态流水线转向**自主代理 (Autonomous Agents)** 。这些代理使用 **大型推理模型 (LRM)**，模拟人类解决复杂问题的“研究”过程。(通过使模型能够根据来自环境或人类的实时反馈来决定其下一步行动来实现的)

本节将从以下四个方面全面介绍搜索代理的研究:
1 搜索代理的架构  2 信息检索模块 3 搜索代理的优化 4 基准测试和资源

### 7.1 architecture of Search Agent
现有方法大致分为两类:单智能体和多智能体(是否分配职责)

#### 7.1.1 single-agent frameworks
**单代理 (Single-Agent)**：一个 LLM 负责所有工作（推理、交互、生成）。通常采用 **ReAct** 风格（Think, Search, Answer）
		*multi-hop question 复杂的多跳问题*
	采用RL --> ==GRPO算法==广泛用于提升性能 --> r1-searcher


简洁性，易于优化--> 难以处理高度复杂的查询(需要用到大量工具和长上下文)

#### 7.1.2 multi-agent frameworks
**多代理 (Multi-Agent)**：将任务分解，由多个专门的 LLM 协同工作 。例如，一个“**规划者 (Planner)**”代理负责分解任务，多个“**搜索者 (Searchers)**”代理负责执行检索 。

允许各个代理专注于不同的任务，从而提高系统整体效率
**然而，通过强化学习联合优化多智能体带来了挑战，目前通常仅限制优化在核心规划智能体上** ->前景:开发更好的强化学习策略


### 7.2 information seeking module

- **基于 API (API-based)**：调用谷歌、必应等搜索引擎 API 。
    
- **基于浏览 (Browsing-based)**：在沙盒或虚拟浏览器环境中模拟人类的**滚动、点击**等交互行为

都有缺点:
	基于浏览虽然更适合检索实时和深度嵌套内容，但是通常产生更高延迟和资源成本
	基于api的方法简单易用，但是难以处理javascript渲染的复杂动态内容，交互式组件或者需要身份验证才能访问的信息
近期研究:ZeroSearch [305] 训练 LLM 来模拟搜索引擎的行为，
而无需实际调用 API，从而显著降低训练成本。
此外，一些方法，例如 Alita [295]，提出在智能体的推理过程中动态创建新的MCP 工具，从而实现自我演化能力，减少对预定义工具包的依赖，并进一步优化计算成本。

### 7.3 Optimization
- **策略性检索 (Strategic Retrieval)**：训练代理学会**判断“何时”检索**，何时使用内部知识 。
    
- **迭代检索 (Iterative Retrieval)**：通过监督学习（SFT）或强化学习（RL） 来训练代理执行多步搜索。
    
- **自主开放网络搜索 (Autonomous Open-Web Search)**：在真实的浏览器环境中进行端到端 RL 训练，使其能处理噪声和冲突信息 。
### 7.4 benchmarks and resources
- **QA 导向**：如 HotpotQA（多跳问答）。
    
- **任务导向**：如 GAIA、SWE-bench，评估代理的规划和工具使用能力 

## 8 future direction
### 8.1 Query Rewriter
 1 Rewriting queries according to ranking performance
	 尽管 LLM能够识别查询的潜在用户意图[338 S. MacAvaney, C. Macdonald, R. Murray-Smith, and I. Ounis, “Intent5: Search result diversification using causal language models,” CoRR, vol. abs/2108.04026, 2021.]，但它们缺乏对重写查询的检索质量的感知。这种缺失可能导致重写后的查询看似正确，但排名结果缺不尽如人意。虽然一些现有研究已使用RL根据结果调整，但**如何整合排名结果**仍有大量研究领域尚未探索
	 
	 **这种缺失导致的结果是：** Rewriter 可能会过度修改查询，引入检索系统不理解的噪声，使得查询的语义上更丰富，但实际检索性能却更差。
2 Achieving personalized query rewriter
	利用 LLM 分析用户搜索历史，构建用户画像，实现**个性化的查询重写**

### 8.2 Retriever
- **延迟 (Latency)**：**知识蒸馏 (Knowledge Distillation)** 和**量化 (Quantization)** 是降低 LLM 检索器延迟的关键方向 。
    
- **生成式检索**：如何为生成式检索器（基于 DocID）设计**增量索引 (Incremental Indexing)**？（即当新文档加入时，如何高效更新模型参数）。 llm参数有静态特性，加上昂贵的微调成本。
    
- **多模态 (Multi-modal)**：将 LLM 的语言理解能力与现有的多模态检索模型结合，支持图文视频混合搜索 。而不是直接用多模态大模型替换骨干网络
### 8.3 Re-ranker
- **效率**：研究如何**蒸馏 (Distill)** LLM（如 GPT-4）的排序能力到一个更小、更高效的模型中 。
    
- **多样性**：让 LLM 适应多样的排序任务，如**响应排序 (Response Ranking)**、实体排序、证据排序等 。(universal info access system) 论文原话:intriguing and valuable
- 个性化:同重写器
### 8.4 Reader-RAG
- **提升参考质量**：RAG 的效果很大程度上取决于检索质量。研究方向是**更好的片段抽取 (Snippet Extraction)**，而不是喂给 LLM 整个（可能充满噪声的）文档 。
    
- **可靠性 (Reliability)**：如何**确保** LLM 在生成答案时**忠实地使用了参考文献**？这是缓解幻觉的核心挑战 。
### 8.5 Agent
**可信度 (Trustworthiness)**：如何让代理学会**自主验证 (Self-Validation)** 从网络上搜集到的信息，评估其可信度 。
	文档的可信度，是否真的用于生成响应(这些问题今天依然很严重！)
	
**Mitigating Bias and Offensive Content in LLMs:** 没啥好说的

### 8.6 Evaluation ==1==
- **新指标**：传统的 IR 指标（如 nDCG）已不够用。需要“**面向生成的排序评估 (Generation-oriented ranking evaluation)**”——即评估一个文档是否“**适合生成答案**”，而不仅仅是“主题相关” 。
	 文本生成评估(1)依赖词汇匹配(没有创意和新颖的内容)
				(2)细微差异不敏感(3)幻觉
    
- **评估幻觉**：需要超越 ROUGE/BLEU(依赖词汇匹配,基于 n-gram 匹配,这种方法无法考虑词汇多样性和上下文语义)，开发能**评估事实一致性 (Factuality)** 的新指标 。
		引入知识库解决幻觉

### 8.7 bias
一个非常新颖的发现：神经检索器存在一种“**来源偏见 (Source Bias)**”，它们**更偏好 LLM 生成的文本**（因为更流畅、主题更集中），而不是人类写的文本 。这是一个亟待解决的公平性问题。

Ai-generated images introduce invisible relevance bias to text-image retrieval
Llms may dominate information access: Neural retrievers are biased towards llmgenerated texts
指出IR 系统中的一些模块，例如检索器和重排序器，尤其是基于神经网络模型的模块，可能更倾向于 LLM 生成的文档、

## 9 conclusion



eva
coT
按需索取  不足&设计
